{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a712d7e9",
   "metadata": {
    "id": "a712d7e9"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d7b73d4f",
   "metadata": {
    "id": "d7b73d4f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "import ast\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from numpy.fft import fft, fftfreq\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Concatenate, Activation, Add, GlobalAveragePooling1D, \\\n",
    "    Dense, LSTM, TimeDistributed, Reshape, BatchNormalization, Bidirectional, Flatten, MaxPooling1D, Dropout, \\\n",
    "    SeparableConv1D\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "LE_MAGIC_NUM = 42\n",
    "rng = np.random.default_rng(LE_MAGIC_NUM)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(LE_MAGIC_NUM)\n",
    "\n",
    "import random\n",
    "random.seed(LE_MAGIC_NUM)\n",
    "\n",
    "np.random.seed(LE_MAGIC_NUM)\n",
    "\n",
    "tf.random.set_seed(LE_MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68f94e7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68f94e7b",
    "outputId": "820a3036-4e3c-416c-e9d6-40727fc994e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f6da239",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f6da239",
    "outputId": "25142019-f9b5-4613-f58d-00fc18a97aac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b31107",
   "metadata": {
    "id": "20b31107"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31f74ed5",
   "metadata": {
    "id": "31f74ed5"
   },
   "outputs": [],
   "source": [
    "def sensitivity_specificity(cf_matrix):\n",
    "  if cf_matrix.shape[0] == 1:\n",
    "    return np.array([1, 1])\n",
    "  TN = cf_matrix[0][0]\n",
    "  FP = cf_matrix[0][1]\n",
    "  TP = cf_matrix[1][1]\n",
    "  FN = cf_matrix[1][0]\n",
    "\n",
    "  sensitivity = TP/(TP+FN) if TP & FN else 0\n",
    "  specificity = TN/(TN+FP) if TN & FP else 0\n",
    "  return sensitivity, specificity\n",
    "\n",
    "def plot_history(history):\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  plt.figure(figsize=(20,4))\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(epochs, acc, label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation acc')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(epochs, loss, label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def draw_confusion_matrix(cf_matrix):\n",
    "\n",
    "  df_cm = pd.DataFrame(cf_matrix, index = [i for i in \"01\"],\n",
    "                columns = [i for i in \"01\"])\n",
    "  ax = plt.subplot()\n",
    "  sns.heatmap(df_cm, annot=True, fmt = \".1f\", ax=ax)\n",
    "  ax.set_xlabel('Predicted')\n",
    "  ax.set_ylabel('True')\n",
    "  TN = cf_matrix[0][0]\n",
    "  FP = cf_matrix[0][1]\n",
    "  TP = cf_matrix[1][1]\n",
    "  FN = cf_matrix[1][0]\n",
    "  print(f'Sensitivity: {TP/(TP+FN)}, Specificity: {TN/(TN+FP)}\\n')\n",
    "  # print(f'True positive samples: {TP} (These are the FOG samples we classified FOG)')\n",
    "  # print(f'False positive samples: {FP} (These are normal samples we classified as FOG)')\n",
    "  # print(f'True negative samples: {TN} (These are the normal samples we classified as normal)')\n",
    "  # print(f'False negative samples: {FN} (These are the FOG samples we classified as normal)')\n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "\n",
    "def segment(x, y, window_size, dataset_signals=9):\n",
    "    #     print(f\"X: {x.shape}\\nY: {y.shape}\\nWin_size: {window_size}\\nSignals: {dataset_signals}\")\n",
    "    segments = np.zeros(((len(x) // (window_size // 2)) - 1, window_size, dataset_signals))\n",
    "    labels = np.zeros(((len(y) // (window_size // 2)) - 1))\n",
    "    i_segment = 0\n",
    "    i_label = 0\n",
    "    for (start, end) in windows(x, window_size):\n",
    "        if len(x[start:end]) == window_size:\n",
    "            m = stats.mode(y[start:end], keepdims=False)\n",
    "            segments[i_segment] = x[start:end]\n",
    "            labels[i_label] = m[0]\n",
    "            i_label += 1\n",
    "            i_segment += 1\n",
    "    return segments, labels\n",
    "\n",
    "def transform_y(y, nr_classes):\n",
    "    # Transforms y, a list with one sequence of A timesteps\n",
    "    # and B unique classes into a binary Numpy matrix of\n",
    "    # shape (A, B)\n",
    "    ybinary = to_categorical(y, nr_classes)\n",
    "    return ybinary\n",
    "\n",
    "# CNN-LSTM Model\n",
    "def cnn_lstm(x_shape,\n",
    "             n_classes,\n",
    "             n_hidden=128,\n",
    "             learning_rate=0.01,\n",
    "             n_steps=4,\n",
    "             length=32,\n",
    "             n_signals=9,\n",
    "             regularization_rate=0.01,\n",
    "             cnn_depth=3,\n",
    "             lstm_depth=2,\n",
    "             metrics=['accuracy']):\n",
    "    \"\"\" CNN1D_LSTM version 1: Divide 1 window into several smaller frames, then apply CNN to each frame\n",
    "    - Input data format: [None, n_frames, n_timesteps, n_signals]\"\"\"\n",
    "\n",
    "    _input_shape = x_shape[1:]\n",
    "    m = Sequential()\n",
    "\n",
    "    m.add(Reshape((n_steps, length, n_signals), input_shape=_input_shape))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')))\n",
    "    m.add(TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu')))\n",
    "    m.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    m.add(TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu')))\n",
    "    m.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
    "    m.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    m.add(TimeDistributed(Flatten()))\n",
    "    for _ in range(lstm_depth-1):\n",
    "        m.add(LSTM(n_hidden, return_sequences=True,\n",
    "                   kernel_regularizer=l2(regularization_rate)))\n",
    "    m.add(LSTM(n_hidden))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(100, activation='relu',\n",
    "                kernel_regularizer=l2(regularization_rate)))\n",
    "    m.add(Dense(n_classes, activation=out_activ))\n",
    "\n",
    "    m.compile(loss=out_loss,\n",
    "              optimizer=Adam(learning_rate=learning_rate, amsgrad=True),\n",
    "              metrics=metrics)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Vanilla LSTM Model\n",
    "def vanilla_lstm(x_shape,\n",
    "                 n_classes,\n",
    "                 n_hidden=128,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_rate=0.01,\n",
    "                 metrics=['accuracy']):\n",
    "    \"\"\" Requires 3D data: [n_samples, n_timesteps, n_signals]\"\"\"\n",
    "    _input_shape = x_shape[1:]\n",
    "    m = Sequential()\n",
    "\n",
    "    m.add(BatchNormalization(input_shape=_input_shape))\n",
    "    m.add(LSTM(n_hidden))\n",
    "    m.add(Dropout(0.3))\n",
    "    m.add(Dense(100, activation='relu'))\n",
    "    m.add(Dense(n_classes, activation=out_activ, kernel_regularizer=l2(regularization_rate)))\n",
    "\n",
    "    m.compile(loss=out_loss,\n",
    "              optimizer=Adam(learning_rate=learning_rate),\n",
    "              metrics=metrics)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Stacked LSTM Model\n",
    "def stacked_lstm(x_shape,\n",
    "                 n_classes,\n",
    "                 n_hidden=128,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_rate=0.01,\n",
    "                 depth=2,\n",
    "                 metrics=['accuracy']):\n",
    "    \"\"\" Require 3D data: [n_samples, n_timesteps, n_signals]\"\"\"\n",
    "    _input_shape = x_shape[1:]\n",
    "    dim_length = x_shape[1]  # number of samples in a time series\n",
    "    dim_channels = x_shape[2]  # number of channels\n",
    "    dim_output = n_classes\n",
    "    m = Sequential()\n",
    "\n",
    "    m.add(BatchNormalization(input_shape=_input_shape))\n",
    "    m.add(Dense(100, activation='relu', name='preprocess', kernel_regularizer=l2(regularization_rate)))\n",
    "    m.add(LSTM(n_hidden, return_sequences=True, kernel_regularizer=l2(regularization_rate)))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(LSTM(n_hidden))\n",
    "    m.add(Dense(100, activation='relu'))\n",
    "    m.add(Dense(dim_output, activation=out_activ, name=\"output\"))\n",
    "\n",
    "    m.compile(loss=out_loss,\n",
    "              optimizer=Adam(learning_rate=learning_rate, amsgrad=True),\n",
    "              metrics=metrics)\n",
    "    return m\n",
    "\n",
    "\n",
    "# BiLSTM Model\n",
    "def bilstm(x_shape,\n",
    "           n_classes,\n",
    "           n_hidden=128,\n",
    "           learning_rate=0.01,\n",
    "           regularization_rate=0.01,\n",
    "           merge_mode='concat',\n",
    "           depth=2,\n",
    "           metrics=['accuracy']):\n",
    "    \"\"\" Requires 3D data: [n_samples, n_timesteps, n_features]\"\"\"\n",
    "\n",
    "    _input_shape = x_shape[1:]\n",
    "    m = Sequential()\n",
    "\n",
    "    m.add(BatchNormalization(input_shape=_input_shape))\n",
    "    m.add(Bidirectional(LSTM(n_hidden), merge_mode=merge_mode))\n",
    "    m.add(Dense(100, activation='relu', kernel_regularizer=l2(regularization_rate)))\n",
    "    m.add(Dense(n_classes, activation=out_activ))\n",
    "\n",
    "    m.compile(loss=out_loss,\n",
    "              optimizer=Adam(learning_rate=learning_rate, amsgrad=True),\n",
    "              metrics=metrics)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# iSPLInception Model\n",
    "def ispl_inception(x_shape,\n",
    "                   n_classes,\n",
    "                   filters_number,\n",
    "                   network_depth=5,\n",
    "                   use_residual=True,\n",
    "                   use_bottleneck=True,\n",
    "                   max_kernel_size=20,\n",
    "                   learning_rate=0.01,\n",
    "                   bottleneck_size=32,\n",
    "                   regularization_rate=0.01,\n",
    "                   metrics=['accuracy']):\n",
    "    dim_length = x_shape[1]  # number of samples in a time series\n",
    "    dim_channels = x_shape[2]  # number of channels\n",
    "    weightinit = 'lecun_uniform'  # weight initialization\n",
    "\n",
    "    def inception_module(input_tensor, stride=1, activation='relu'):\n",
    "\n",
    "        # The  channel number is greater than 1\n",
    "        if use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = Conv1D(filters=bottleneck_size,\n",
    "                                     kernel_size=1,\n",
    "                                     padding='same',\n",
    "                                     activation=activation,\n",
    "                                     kernel_initializer=weightinit,\n",
    "\n",
    "                                     use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        kernel_sizes = [max_kernel_size // (2 ** i) for i in range(3)]\n",
    "        conv_list = []\n",
    "\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv_list.append(Conv1D(filters=filters_number,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    strides=stride,\n",
    "                                    padding='same',\n",
    "                                    activation=activation,\n",
    "                                    kernel_initializer=weightinit,\n",
    "                                    kernel_regularizer=l2(regularization_rate),\n",
    "                                    use_bias=False)(input_inception))\n",
    "\n",
    "        max_pool_1 = MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_last = Conv1D(filters=filters_number,\n",
    "                           kernel_size=1,\n",
    "                           padding='same',\n",
    "                           activation=activation,\n",
    "                           kernel_initializer=weightinit,\n",
    "                           kernel_regularizer=l2(regularization_rate),\n",
    "                           use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_last)\n",
    "\n",
    "        x = Concatenate(axis=2)(conv_list)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def shortcut_layer(input_tensor, out_tensor):\n",
    "        shortcut_y = Conv1D(filters=int(out_tensor.shape[-1]),\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            kernel_initializer=weightinit,\n",
    "                            kernel_regularizer=l2(regularization_rate),\n",
    "                            use_bias=False)(input_tensor)\n",
    "        shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = Add()([shortcut_y, out_tensor])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    # Build the actual model:\n",
    "    input_layer = Input((dim_length, dim_channels))\n",
    "    x = BatchNormalization()(input_layer)  # Added batchnorm (not in original paper)\n",
    "    input_res = x\n",
    "\n",
    "    for depth in range(network_depth):\n",
    "        x = inception_module(x)\n",
    "\n",
    "        if use_residual and depth % 3 == 2:\n",
    "            x = shortcut_layer(input_res, x)\n",
    "            input_res = x\n",
    "\n",
    "    gap_layer = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Final classification layer\n",
    "    output_layer = Dense(n_classes, activation=out_activ,\n",
    "                         kernel_initializer=weightinit, kernel_regularizer=l2(regularization_rate))(gap_layer)\n",
    "\n",
    "    # Create model and compile\n",
    "    m = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    m.compile(loss=out_loss,\n",
    "              optimizer=Adam(learning_rate=learning_rate, amsgrad=True),\n",
    "              metrics=metrics)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5d799",
   "metadata": {
    "id": "86b5d799"
   },
   "source": [
    "# Read and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc173325",
   "metadata": {
    "id": "bc173325"
   },
   "source": [
    "Reading txt files as dataframes, adding headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "543ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset='daphnet', datapath='daphnet', _type='original'):\n",
    "    datapath = datapath.rstrip(\"/\")\n",
    "    # Check if our desired dataset has not yet been generated\n",
    "    if not os.path.exists(f'{datapath}/{dataset}.h5'):\n",
    "        DataReader(dataset, datapath)\n",
    "\n",
    "    # class labels\n",
    "    with open(f'{datapath}/{dataset}.h5.classes.json', 'r') as f:\n",
    "        labels = ast.literal_eval(f.read())\n",
    "\n",
    "    with h5py.File(f'{datapath}/{dataset}.h5', 'r') as f:\n",
    "        X_train, y_train = np.array(f['train']['inputs']), np.array(f['train']['targets'])\n",
    "        X_val, y_val = np.array(f['validation']['inputs']), np.array(f['validation']['targets'])\n",
    "        X_test, y_test = np.array(f['test']['inputs']), np.array(f['test']['targets'])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, labels\n",
    "\n",
    "X_train, y_train_int, X_val, y_val_int, X_test, y_test_int, labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4f101f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 9  # Ankle acc - x,y,z; Upper Leg acc - x,y,z; Trunk acc - x,y,z;\n",
    "win_size = 192  # 1 sec -> 64 | 2.56 (3) sec -> 192 | 5 sec -> 320 # sampling rate = 64hz\n",
    "n_classes = 2  # 1 - no freeze (walk, stand, turn); 2 - freeze\n",
    "n_steps = 3  # Since we are using 3 seconds and 192 is divisible by 3\n",
    "length = 64  # Split each window of 192 time steps into sub sequences for the cnn\n",
    "\n",
    "X_train, y_train_int = segment(X_train, y_train_int, win_size, dataset_signals=n_signals)\n",
    "X_val, y_val_int = segment(X_val, y_val_int, win_size, dataset_signals=n_signals)\n",
    "X_test, y_test_int = segment(X_test, y_test_int, win_size, dataset_signals=n_signals)\n",
    "\n",
    "y_train = transform_y(y_train_int, n_classes)\n",
    "y_val = transform_y(y_val_int, n_classes)\n",
    "y_test = transform_y(y_test_int, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93c7e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_5 (Batc  (None, 192, 9)           36        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               70656     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               12900     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,794\n",
      "Trainable params: 83,776\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape\n",
    "\n",
    "training_id = f'{time():.0f}'\n",
    "out_loss = 'binary_crossentropy'\n",
    "out_activ = 'sigmoid'\n",
    "model_name = 'vanilla_lstm'\n",
    "METRICS = ['accuracy']\n",
    "save_name = os.path.abspath(f\"models/daphnet/{training_id}/{model_name}.h5\")\n",
    "log_dir = os.path.abspath(f'logs/fit/daphnet/{training_id}/{model_name}')\n",
    "\n",
    "if model_name == 'vanilla_lstm':\n",
    "    # Give model specific configurations\n",
    "    hyperparameters = {'n_hidden': 128, 'learning_rate': 0.0005, 'regularization_rate': 0.000093}\n",
    "    epochs = 350\n",
    "    # batch_size = 64\n",
    "    mod_name = 'vLSTM'\n",
    "    patience = 300\n",
    "\n",
    "elif model_name == 'stacked_lstm':\n",
    "    # Give model specific configurations\n",
    "    hyperparameters = {'n_hidden': 128, 'learning_rate': 0.0005, 'regularization_rate': 0.000093,\n",
    "                       'depth': 4}\n",
    "    mod_name = 'sLSTM'\n",
    "    epochs = 350\n",
    "    patience = 150\n",
    "\n",
    "elif model_name == 'bilstm':\n",
    "    # Give model specific configurations\n",
    "    hyperparameters = {'n_hidden': 128, 'learning_rate': 0.0005, 'regularization_rate': 0.000093,\n",
    "                       'depth': 1,\n",
    "                       'merge_mode': 'concat'}\n",
    "    mod_name = 'BiLSTM'\n",
    "    epochs = 350\n",
    "    patience = 200\n",
    "\n",
    "elif model_name == 'cnn':\n",
    "    # Give model specific configurations\n",
    "    hyperparameters = {'filters': [32, 64, 32], 'fc_hidden_nodes': 100, 'learning_rate': 0.0005,\n",
    "                       'regularization_rate': 0.000093}\n",
    "    mod_name = 'CNN'\n",
    "    epochs = 350\n",
    "    patience = 300\n",
    "\n",
    "elif model_name == 'cnn_lstm':\n",
    "    # Give model specific configurations\n",
    "    hyperparameters = {'n_hidden': 512, 'n_steps': n_steps, 'length': length, 'n_signals': n_signals,\n",
    "                       'learning_rate': 0.0005, 'cnn_depth': 3, 'lstm_depth': 2,\n",
    "                       'regularization_rate': 0.000093}\n",
    "    mod_name = 'CNN_LSTM'\n",
    "    epochs = 350\n",
    "    patience = 300\n",
    "\n",
    "else:\n",
    "    # Give default model specific configurations\n",
    "    # Our default model is the iSPLInception model\n",
    "    use_residual = True\n",
    "    use_bottleneck = True\n",
    "    hyperparameters = {'learning_rate': 0.0005, 'regularization_rate': 0.00593,\n",
    "                       'network_depth': 5, 'filters_number': 64, 'max_kernel_size': 68,\n",
    "                       'use_residual': use_residual, 'use_bottleneck': use_bottleneck}\n",
    "    mod_name = 'iSPLInception'\n",
    "    epochs = 350\n",
    "    patience = 300\n",
    "\n",
    "# Create and initialize the Model\n",
    "model = eval(model_name + f\"(input_shape, n_classes, metrics=METRICS, **hyperparameters)\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7bcefdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "133/133 [==============================] - 10s 44ms/step - loss: 0.3768 - accuracy: 0.8649 - val_loss: 0.6081 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 2/350\n",
      "133/133 [==============================] - 6s 42ms/step - loss: 0.3020 - accuracy: 0.8819 - val_loss: 0.5530 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 3/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.3020 - accuracy: 0.8820 - val_loss: 0.6765 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 4/350\n",
      "133/133 [==============================] - 5s 40ms/step - loss: 0.2851 - accuracy: 0.8830 - val_loss: 0.5989 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 5/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2938 - accuracy: 0.8800 - val_loss: 0.6006 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 6/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2850 - accuracy: 0.8834 - val_loss: 0.5812 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 7/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2712 - accuracy: 0.8864 - val_loss: 0.6949 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 8/350\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.2753 - accuracy: 0.8869 - val_loss: 0.6413 - val_accuracy: 0.7246 - lr: 5.0000e-04\n",
      "Epoch 9/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2855 - accuracy: 0.8818 - val_loss: 0.5462 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 10/350\n",
      "133/133 [==============================] - 5s 37ms/step - loss: 0.2698 - accuracy: 0.8826 - val_loss: 0.6475 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 11/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2581 - accuracy: 0.8918 - val_loss: 0.7076 - val_accuracy: 0.7402 - lr: 5.0000e-04\n",
      "Epoch 12/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2563 - accuracy: 0.8854 - val_loss: 0.6880 - val_accuracy: 0.7227 - lr: 5.0000e-04\n",
      "Epoch 13/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2686 - accuracy: 0.8821 - val_loss: 0.6362 - val_accuracy: 0.7344 - lr: 5.0000e-04\n",
      "Epoch 14/350\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.2775 - accuracy: 0.8834 - val_loss: 0.6529 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 15/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2777 - accuracy: 0.8836 - val_loss: 0.5885 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 16/350\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.2745 - accuracy: 0.8871 - val_loss: 0.5714 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 17/350\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.2603 - accuracy: 0.8850 - val_loss: 0.6026 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 18/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2750 - accuracy: 0.8819 - val_loss: 0.6364 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 19/350\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.2635 - accuracy: 0.8836 - val_loss: 1.1461 - val_accuracy: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 20/350\n",
      " 70/133 [==============>...............] - ETA: 2s - loss: 0.2414 - accuracy: 0.8906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [114], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m load_model(checkpoint_path)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model, history\n\u001b[1;32m---> 37\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43m_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_save_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_log_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [114], line 25\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(_model, _X_train, _y_train, _X_test, _y_test, _epochs, patience, batch_size, _save_name, _log_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     22\u001b[0m                               min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_X_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m_y_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_X_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# shuffle=True,\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m                     \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_monitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m load_model(checkpoint_path)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model, history\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(_model, _X_train, _y_train, _X_test, _y_test, _epochs=20, patience=10,\n",
    "                   batch_size=64, _save_name='models/please_provide_a_name.h5', _log_dir='logs/fit'):\n",
    "    \"\"\"\n",
    "    Returns the best trained model and history objects of the currently provided train & test set\n",
    "    \"\"\"\n",
    "    early_stopping_monitor = EarlyStopping(patience=patience)\n",
    "\n",
    "    checkpoint_path = _save_name\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create checkpoint callback\n",
    "    cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                                  monitor='val_loss',\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=False,\n",
    "                                  verbose=0)\n",
    "    # Tensorboard Callback\n",
    "    tensorboard_callback = TensorBoard(log_dir=_log_dir, histogram_freq=1)\n",
    "\n",
    "    # Reduce Learning rate after plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=10,\n",
    "                                  min_lr=0.0001, verbose=1)\n",
    "\n",
    "    # Training the model\n",
    "    history = _model.fit(_X_train,\n",
    "                         _y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         validation_data=(_X_test, _y_test),\n",
    "                         epochs=_epochs,\n",
    "                         verbose=1,\n",
    "                         # shuffle=True,\n",
    "                         use_multiprocessing=True,\n",
    "                         callbacks=[cp_callback, tensorboard_callback, early_stopping_monitor, reduce_lr])\n",
    "    best_model = load_model(checkpoint_path)\n",
    "    return best_model, history\n",
    "\n",
    "model, history = evaluate_model(model, X_train, y_train, X_val, y_val, patience=patience,\n",
    "                                                    _epochs=epochs, _save_name=save_name, _log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66d7206f",
   "metadata": {
    "id": "66d7206f"
   },
   "outputs": [],
   "source": [
    "dataframes = dict(zip(range(1,11), [list() for x in range(1,11)]))\n",
    "path = 'daphnet/dataset'\n",
    "file_names = os.listdir(path)\n",
    "col_names = ['time', 'ank_hor_fwd', 'ank_vert', 'ank_hor_lat', 'leg_hor_fwd', 'leg_vert', 'leg_hor_lat', \n",
    "             'trunk_hor_fwd', 'trunk_vert', 'trunk_hor_lat', 'label']\n",
    "feature_col_names = col_names[1:10]\n",
    "\n",
    "for name in file_names:\n",
    "    splits = name.split('R')\n",
    "    num = int(splits[0][1:])\n",
    "    df = pd.read_csv(os.path.join(path,name),  delimiter=' ', names=col_names, header=None)\n",
    "\n",
    "    # Remove instances where session hasn't started and decrement all labels by 1\n",
    "    df = df[df['label'] != 0]\n",
    "    df['label'] -= 1\n",
    "\n",
    "    dataframes[num].append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9ccdc",
   "metadata": {
    "id": "d7c9ccdc"
   },
   "source": [
    "Remove patients 4 and 10 without any freezing episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b1353e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72b1353e",
    "outputId": "f9a7696c-1ef0-4398-820b-895461d93257"
   },
   "outputs": [],
   "source": [
    "# dataframes.pop(4)\n",
    "# dataframes.pop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbbfae6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6bbbfae6",
    "outputId": "ec07bd42-69c1-4409-c185-3080f51bb029"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>ank_hor_fwd</th>\n",
       "      <th>ank_vert</th>\n",
       "      <th>ank_hor_lat</th>\n",
       "      <th>leg_hor_fwd</th>\n",
       "      <th>leg_vert</th>\n",
       "      <th>leg_hor_lat</th>\n",
       "      <th>trunk_hor_fwd</th>\n",
       "      <th>trunk_vert</th>\n",
       "      <th>trunk_hor_lat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>750000</td>\n",
       "      <td>-30</td>\n",
       "      <td>990</td>\n",
       "      <td>326</td>\n",
       "      <td>-45</td>\n",
       "      <td>972</td>\n",
       "      <td>181</td>\n",
       "      <td>-38</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48000</th>\n",
       "      <td>750015</td>\n",
       "      <td>-30</td>\n",
       "      <td>1000</td>\n",
       "      <td>356</td>\n",
       "      <td>-18</td>\n",
       "      <td>981</td>\n",
       "      <td>212</td>\n",
       "      <td>-48</td>\n",
       "      <td>1028</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48001</th>\n",
       "      <td>750031</td>\n",
       "      <td>-20</td>\n",
       "      <td>990</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>981</td>\n",
       "      <td>222</td>\n",
       "      <td>-38</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48002</th>\n",
       "      <td>750046</td>\n",
       "      <td>-20</td>\n",
       "      <td>1000</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>222</td>\n",
       "      <td>-19</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48003</th>\n",
       "      <td>750062</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>212</td>\n",
       "      <td>-29</td>\n",
       "      <td>1038</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143995</th>\n",
       "      <td>2249937</td>\n",
       "      <td>-212</td>\n",
       "      <td>921</td>\n",
       "      <td>425</td>\n",
       "      <td>800</td>\n",
       "      <td>370</td>\n",
       "      <td>212</td>\n",
       "      <td>126</td>\n",
       "      <td>923</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143996</th>\n",
       "      <td>2249953</td>\n",
       "      <td>-202</td>\n",
       "      <td>911</td>\n",
       "      <td>405</td>\n",
       "      <td>809</td>\n",
       "      <td>379</td>\n",
       "      <td>202</td>\n",
       "      <td>135</td>\n",
       "      <td>923</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143997</th>\n",
       "      <td>2249968</td>\n",
       "      <td>-222</td>\n",
       "      <td>921</td>\n",
       "      <td>405</td>\n",
       "      <td>809</td>\n",
       "      <td>379</td>\n",
       "      <td>202</td>\n",
       "      <td>135</td>\n",
       "      <td>904</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143998</th>\n",
       "      <td>2249984</td>\n",
       "      <td>-222</td>\n",
       "      <td>921</td>\n",
       "      <td>405</td>\n",
       "      <td>818</td>\n",
       "      <td>388</td>\n",
       "      <td>232</td>\n",
       "      <td>106</td>\n",
       "      <td>923</td>\n",
       "      <td>446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143999</th>\n",
       "      <td>2250000</td>\n",
       "      <td>-212</td>\n",
       "      <td>921</td>\n",
       "      <td>435</td>\n",
       "      <td>818</td>\n",
       "      <td>370</td>\n",
       "      <td>222</td>\n",
       "      <td>135</td>\n",
       "      <td>933</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92802 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "47999    750000          -30       990          326          -45       972   \n",
       "48000    750015          -30      1000          356          -18       981   \n",
       "48001    750031          -20       990          336           18       981   \n",
       "48002    750046          -20      1000          316           36       990   \n",
       "48003    750062            0       990          316           36       990   \n",
       "...         ...          ...       ...          ...          ...       ...   \n",
       "143995  2249937         -212       921          425          800       370   \n",
       "143996  2249953         -202       911          405          809       379   \n",
       "143997  2249968         -222       921          405          809       379   \n",
       "143998  2249984         -222       921          405          818       388   \n",
       "143999  2250000         -212       921          435          818       370   \n",
       "\n",
       "        leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "47999           181            -38        1000             29      0  \n",
       "48000           212            -48        1028             29      0  \n",
       "48001           222            -38        1038              9      0  \n",
       "48002           222            -19        1038              9      0  \n",
       "48003           212            -29        1038             29      0  \n",
       "...             ...            ...         ...            ...    ...  \n",
       "143995          212            126         923            417      0  \n",
       "143996          202            135         923            436      0  \n",
       "143997          202            135         904            436      0  \n",
       "143998          232            106         923            446      0  \n",
       "143999          222            135         933            417      0  \n",
       "\n",
       "[92802 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82974164",
   "metadata": {
    "id": "82974164"
   },
   "source": [
    "# Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c00f9a3",
   "metadata": {
    "id": "8c00f9a3"
   },
   "outputs": [],
   "source": [
    "dfs_no_outliers = dict(zip(dataframes.keys(), [list() for x in dataframes.keys()]))\n",
    "\n",
    "for i, subject in dataframes.items():\n",
    "    for j, session in enumerate(subject):\n",
    "        features = session[feature_col_names]\n",
    "        \n",
    "        for col in features.columns:\n",
    "            features[col][\n",
    "              (features[col] < features[col].median(axis=0) - features[col].std(axis=0) * 4) |\n",
    "              (features[col] > features[col].median(axis=0) + features[col].std(axis=0) * 4)\n",
    "            ] = features[col].median()\n",
    "        session.loc[:, feature_col_names] = features\n",
    "        dfs_no_outliers[i].append(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1e5758",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb1e5758",
    "outputId": "e029e2bf-a116-4d5e-bedf-3efad9f3c7f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  47999    750000          -30       990          326          -45       972   \n",
       "  48000    750015          -30      1000          356          -18       981   \n",
       "  48001    750031          -20       990          336           18       981   \n",
       "  48002    750046          -20      1000          316           36       990   \n",
       "  48003    750062            0       990          316           36       990   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  143995  2249937         -212       921          425          800       370   \n",
       "  143996  2249953         -202       911          405          809       379   \n",
       "  143997  2249968         -222       921          405          809       379   \n",
       "  143998  2249984         -222       921          405          818       388   \n",
       "  143999  2250000         -212       921          435          818       370   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  47999           181            -38        1000             29      0  \n",
       "  48000           212            -48        1028             29      0  \n",
       "  48001           222            -38        1038              9      0  \n",
       "  48002           222            -19        1038              9      0  \n",
       "  48003           212            -29        1038             29      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  143995          212            126         923            417      0  \n",
       "  143996          202            135         923            436      0  \n",
       "  143997          202            135         904            436      0  \n",
       "  143998          232            106         923            446      0  \n",
       "  143999          222            135         933            417      0  \n",
       "  \n",
       "  [92802 rows x 11 columns],\n",
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  15999  250000         -161      1029           99         -190       981   \n",
       "  16000  250015         -131      1019           89         -136       972   \n",
       "  16001  250031         -121      1019           99         -100       981   \n",
       "  16002  250046         -111      1029           69          -81       981   \n",
       "  16003  250062         -101      1019           59          -81       981   \n",
       "  ...       ...          ...       ...          ...          ...       ...   \n",
       "  44795  699937         -151      1000          247          836       472   \n",
       "  44796  699953         -151      1019          227          836       472   \n",
       "  44797  699968         -171      1000          217          827       453   \n",
       "  44798  699984         -151      1000          217          809       490   \n",
       "  44799  700000         -171      1009          198          818       472   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  15999          171             58        1028            116      0  \n",
       "  16000          151             38        1000            106      0  \n",
       "  16001          111              9         990            155      0  \n",
       "  16002           90             29        1000            126      0  \n",
       "  16003           90              0         990            155      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  44795          -70            436         828            388      0  \n",
       "  44796          -70            427         838            407      0  \n",
       "  44797          -80            417         828            398      0  \n",
       "  44798         -121            427         838            388      0  \n",
       "  44799         -121            427         838            398      0  \n",
       "  \n",
       "  [28801 rows x 11 columns]],\n",
       " 2: [          time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  42879   670000         -151       990          267           63       990   \n",
       "  42880   670015         -151      1000          277           63       981   \n",
       "  42881   670031         -181      1009          247           63       981   \n",
       "  42882   670046         -161       990          237           81       981   \n",
       "  42883   670062         -151      1000          257           81       981   \n",
       "  ...        ...          ...       ...          ...          ...       ...   \n",
       "  68475  1069937         -686       754          188          745       500   \n",
       "  68476  1069953         -686       764          188          745       490   \n",
       "  68477  1069968         -686       754          168          736       490   \n",
       "  68478  1069984         -676       735          168          736       490   \n",
       "  68479  1070000         -696       764          178          754       490   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  42879           80             -9        1019              0      0  \n",
       "  42880           80             -9        1009             58      0  \n",
       "  42881           70             -9        1019             38      0  \n",
       "  42882           80              0        1019             48      0  \n",
       "  42883           80             -9        1000             38      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  68475          333            -77        1019            135      0  \n",
       "  68476          343            -77        1009            155      0  \n",
       "  68477          343            -58        1000            135      0  \n",
       "  68478          323            -77        1009            126      0  \n",
       "  68479          333            -58        1000            126      0  \n",
       "  \n",
       "  [25601 rows x 11 columns],\n",
       "            time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  11839   185000         -111      1000          237          -18       990   \n",
       "  11840   185015         -121      1000          237            0       990   \n",
       "  11841   185031         -111      1009          237           -9       981   \n",
       "  11842   185046         -121      1000          237           -9       981   \n",
       "  11843   185062         -111      1009          227           -9       990   \n",
       "  ...        ...          ...       ...          ...          ...       ...   \n",
       "  76795  1199937         -242       852          534          781       277   \n",
       "  76796  1199953         -242       852          564          781       268   \n",
       "  76797  1199968         -262       862          554          781       268   \n",
       "  76798  1199984         -252       882          534          781       268   \n",
       "  76799  1200000         -252       862          554          763       277   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  11839           90           -155        1019             29      0  \n",
       "  11840           60           -155        1009             38      0  \n",
       "  11841           80           -155        1019             48      0  \n",
       "  11842           80           -155        1009             48      0  \n",
       "  11843           80           -135        1019             19      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  76795          464            -58         952            378      0  \n",
       "  76796          444            -48         952            388      0  \n",
       "  76797          454            -67         952            368      0  \n",
       "  76798          464            -48         952            407      0  \n",
       "  76799          474            -48         942            388      0  \n",
       "  \n",
       "  [64961 rows x 11 columns]],\n",
       " 3: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  42879    670000          -90      1000          257         -245       962   \n",
       "  42880    670015          -70      1000          247         -254       944   \n",
       "  42881    670031          -70       990          267         -245       972   \n",
       "  42882    670046          -80      1009          287         -254       962   \n",
       "  42883    670062          -70      1009          257         -245       953   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  139515  2179937          -10       921          514         -927       296   \n",
       "  139516  2179953          -30       921          504         -918       287   \n",
       "  139517  2179968          -30       921          504         -909       277   \n",
       "  139518  2179984          -10       911          514         -918       277   \n",
       "  139519  2180000          -30       911          514         -918       287   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  42879            10           -135        1019            -87      0  \n",
       "  42880            40           -116         990            -97      0  \n",
       "  42881            50           -126        1009            -87      0  \n",
       "  42882            50           -126        1019            -97      0  \n",
       "  42883            30           -116        1000            -77      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  139515          212            116         942            349      0  \n",
       "  139516          222            106         933            378      0  \n",
       "  139517          202            106         961            368      0  \n",
       "  139518          222            106         952            359      0  \n",
       "  139519          222            116         942            359      0  \n",
       "  \n",
       "  [90882 rows x 11 columns],\n",
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  16639  260000         -434       921          257         -954       129   \n",
       "  16640  260015         -444       921          257         -963       138   \n",
       "  16641  260031         -444       921          257         -963       138   \n",
       "  16642  260046         -414       901          267         -963       138   \n",
       "  16643  260062         -414       901          267         -954       157   \n",
       "  ...       ...          ...       ...          ...          ...       ...   \n",
       "  33275  519937         -353       901          405         -954       166   \n",
       "  33276  519953         -353       901          396         -945       148   \n",
       "  33277  519968         -353       911          415         -945       157   \n",
       "  33278  519984         -363       901          376         -954       157   \n",
       "  33279  520000         -343       892          396         -954       166   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  16639          101             58        1038              0      0  \n",
       "  16640          111             58        1019            -29      0  \n",
       "  16641          121             29        1019            -38      0  \n",
       "  16642          111             29        1019            -38      0  \n",
       "  16643          101             48        1000            -29      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  33275          131            601         809             38      0  \n",
       "  33276          131            601         809             38      0  \n",
       "  33277          131            592         809             38      0  \n",
       "  33278          111            572         819             48      0  \n",
       "  33279          131            601         809             48      0  \n",
       "  \n",
       "  [16641 rows x 11 columns],\n",
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  41599  650000            0      1019          148            0         0   \n",
       "  41600  650015          -20      1019          148            0         0   \n",
       "  41601  650031            0      1039          148            0         0   \n",
       "  41602  650046            0      1019          168            0         0   \n",
       "  41603  650062           10      1019          158            0         0   \n",
       "  ...       ...          ...       ...          ...          ...       ...   \n",
       "  62715  979937           60      1029          168            0         0   \n",
       "  62716  979953           80      1019          178            0         0   \n",
       "  62717  979968           70      1019          188            0         0   \n",
       "  62718  979984           70      1019          188            0         0   \n",
       "  62719  980000           80      1039          178            0         0   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  41599            0             58         990           -194      0  \n",
       "  41600            0             58         980           -174      0  \n",
       "  41601            0             48        1009           -184      0  \n",
       "  41602            0             58        1009           -194      0  \n",
       "  41603            0             67         990           -165      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  62715            0             48         961           -203      0  \n",
       "  62716            0            514         914            -67      0  \n",
       "  62717            0           -165         933            203      0  \n",
       "  62718            0             67        1000           -155      0  \n",
       "  62719            0             67        1000           -155      0  \n",
       "  \n",
       "  [21121 rows x 11 columns]],\n",
       " 4: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  47359    740000          -80      1039          138          145       981   \n",
       "  47360    740015          -80      1039          138          145       981   \n",
       "  47361    740031          -90      1029          108          163       972   \n",
       "  47362    740046          -70      1029          118          145       972   \n",
       "  47363    740062          -70      1039          128          145       990   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  187515  2929937         -252      1019           59          818       472   \n",
       "  187516  2929953         -272      1009           69          809       500   \n",
       "  187517  2929968         -272      1009           69          781       490   \n",
       "  187518  2929984         -252      1000           59          818       481   \n",
       "  187519  2930000         -272      1000           69          809       481   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  47359           -20             -9        1019            -87      0  \n",
       "  47360           -50             -9        1028            -87      0  \n",
       "  47361           -50              0        1009            -77      0  \n",
       "  47362           -50              9        1019            -77      0  \n",
       "  47363           -30             29         990            -77      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  187515           30             87        1009            -58      0  \n",
       "  187516           10             67        1019           -106      0  \n",
       "  187517            0             67        1000            -87      0  \n",
       "  187518          -10             77        1000            -77      0  \n",
       "  187519           10             87        1019            -87      0  \n",
       "  \n",
       "  [132482 rows x 11 columns]],\n",
       " 5: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  28159    440000         -101      1019          198           81       981   \n",
       "  28160    440015         -111      1009          207           72       981   \n",
       "  28161    440031          -90      1009          217           81       981   \n",
       "  28162    440046         -101      1019          217           72       990   \n",
       "  28163    440062          -90      1009          227           90       981   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  105595  1649937          -50       852          653          590       712   \n",
       "  105596  1649953          -20       843          673          618       703   \n",
       "  105597  1649968          -40       843          643          618       685   \n",
       "  105598  1649984          -30       784          673          645       657   \n",
       "  105599  1650000          -50       803          613          590       629   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  28159            80           -106        1019           -116      0  \n",
       "  28160            50            -97        1000            -87      0  \n",
       "  28161            80            -97        1000            -87      0  \n",
       "  28162            70            -97        1009            -87      0  \n",
       "  28163            70           -116        1019            -97      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  105595          585            -38        1000            135      0  \n",
       "  105596          616            -38        1000             29      0  \n",
       "  105597          606            -38        1000              0      0  \n",
       "  105598          555            -38        1000              9      0  \n",
       "  105599          484            -38        1000             58      0  \n",
       "  \n",
       "  [67844 rows x 11 columns],\n",
       "            time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  21759   340000         -131      1029          108          127       990   \n",
       "  21760   340015         -111      1019          128          154       981   \n",
       "  21761   340031         -151      1029          118          136       962   \n",
       "  21762   340046         -111      1029          128          190       962   \n",
       "  21763   340062         -111      1029          128          190       962   \n",
       "  ...        ...          ...       ...          ...          ...       ...   \n",
       "  94075  1469937          343      1009          118          818       481   \n",
       "  94076  1469953          313       990          128          818       527   \n",
       "  94077  1469968          333      1000          108          827       500   \n",
       "  94078  1469984          323      1039          108          836       500   \n",
       "  94079  1470000          333       980          168          836       500   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  21759          -10             19        1057           -116      0  \n",
       "  21760           10             -9        1047           -116      0  \n",
       "  21761          -20              0        1028           -135      0  \n",
       "  21762          -20             -9         990           -135      0  \n",
       "  21763          -20            -19         952           -135      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  94075          -50            864         485            203      0  \n",
       "  94076          -80            844         476            223      0  \n",
       "  94077          -20            864         466            194      0  \n",
       "  94078          -40            854         457            233      0  \n",
       "  94079          -60            873         438            223      0  \n",
       "  \n",
       "  [65922 rows x 11 columns]],\n",
       " 6: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  31999    500000          151      1019          207         -127       953   \n",
       "  32000    500015          131      1019          188         -118       944   \n",
       "  32001    500031          141      1029          188         -118       953   \n",
       "  32002    500046          141      1009          198         -118       944   \n",
       "  32003    500062          151      1000          178         -127       953   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  153595  2399937         -474       833          386         -963        -9   \n",
       "  153596  2399953         -474       833          386         -954        -9   \n",
       "  153597  2399968         -474       833          386         -945       -18   \n",
       "  153598  2399984         -474       833          386         -963       -18   \n",
       "  153599  2400000         -484       823          415         -954        -9   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  31999           222            320         933           -203      0  \n",
       "  32000           232            330         933           -174      0  \n",
       "  32001           212            310         952           -184      0  \n",
       "  32002           232            320         942           -155      0  \n",
       "  32003           232            330         942           -194      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  153595         -151            -97        1009            -19      0  \n",
       "  153596         -151            -97        1019            -48      0  \n",
       "  153597         -141            -87        1000             -9      0  \n",
       "  153598         -161            -87        1009             -9      0  \n",
       "  153599         -151            -97        1028            -19      0  \n",
       "  \n",
       "  [107523 rows x 11 columns],\n",
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  17919  280000          101      1000          297           -9       953   \n",
       "  17920  280015          101      1000          287          -27       944   \n",
       "  17921  280031          121       980          287          -27       935   \n",
       "  17922  280046          111      1000          277          -27       962   \n",
       "  17923  280062          101       980          306          -45       953   \n",
       "  ...       ...          ...       ...          ...          ...       ...   \n",
       "  38395  599937           60      1009           99        -1536      1564   \n",
       "  38396  599953           60      1019          118         -681       500   \n",
       "  38397  599968           20      1088           79          -54       962   \n",
       "  38398  599984           90      1039           79          390       740   \n",
       "  38399  600000           90      1039           79          -81      1527   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  17919          303            330         942           -145      0  \n",
       "  17920          292            310         933           -155      0  \n",
       "  17921          292            300         952           -135      0  \n",
       "  17922          282            320         961           -165      0  \n",
       "  17923          262            310         933           -126      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  38395          252            834         961            145      0  \n",
       "  38396          252            854         961            223      0  \n",
       "  38397          545            844         961            145      0  \n",
       "  38398          595           1048         961            271      0  \n",
       "  38399          -90           1223         961            417      0  \n",
       "  \n",
       "  [19842 rows x 11 columns]],\n",
       " 7: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  38399    600000           50      1019           19         -272       953   \n",
       "  38400    600015           30      1039           19         -263       953   \n",
       "  38401    600031           50      1029           19         -263       953   \n",
       "  38402    600046           50      1029            9         -263       953   \n",
       "  38403    600062           50      1029            0         -281       962   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  112635  1759937          161      1000          217         -836       500   \n",
       "  112636  1759953          161      1009          217         -845       500   \n",
       "  112637  1759968          161       990          237         -836       500   \n",
       "  112638  1759984          161       990          237         -845       490   \n",
       "  112639  1760000          171      1009          198         -845       518   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  38399           -50            300         961            -19      0  \n",
       "  38400           -50            291         961              0      0  \n",
       "  38401           -40            300         971              0      0  \n",
       "  38402           -50            300         971              9      0  \n",
       "  38403           -70            310         952            -19      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  112635            0              0        1019              9      0  \n",
       "  112636          -10              0        1019              9      0  \n",
       "  112637            0              0        1019             38      0  \n",
       "  112638          -10             19        1028             19      0  \n",
       "  112639          -10             -9        1028             29      0  \n",
       "  \n",
       "  [74241 rows x 11 columns],\n",
       "           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  15999  250000          121      1029          108           81       962   \n",
       "  16000  250015          121      1029          108           72       962   \n",
       "  16001  250031          131      1029          118           63       981   \n",
       "  16002  250046          131      1019          118           54       990   \n",
       "  16003  250062          131      1029          118           54       981   \n",
       "  ...       ...          ...       ...          ...          ...       ...   \n",
       "  44795  699937           10      1029          108         -854       481   \n",
       "  44796  699953           30      1049          108         -845       481   \n",
       "  44797  699968           30      1019          108         -854       472   \n",
       "  44798  699984           20      1039          118         -854       472   \n",
       "  44799  700000           10      1039          108         -854       462   \n",
       "  \n",
       "         leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  15999          191             67        1019            -29      0  \n",
       "  16000          181             38        1009            -19      0  \n",
       "  16001          171             87        1009            -38      0  \n",
       "  16002          191             77        1019            -19      0  \n",
       "  16003          171             67        1028            -19      0  \n",
       "  ...            ...            ...         ...            ...    ...  \n",
       "  44795          141           -106         495            -29      0  \n",
       "  44796          121            -97         495            -29      0  \n",
       "  44797          161           -106         495            -29      0  \n",
       "  44798          141           -106         495            -29      0  \n",
       "  44799          151            -87         476            -29      0  \n",
       "  \n",
       "  [28801 rows x 11 columns]],\n",
       " 8: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  39679    620000          101      1009          306         -163      1000   \n",
       "  39680    620015           60      1009          316         -290       925   \n",
       "  39681    620031           40      1000          297         -318       898   \n",
       "  39682    620046           60      1019          287         -254       962   \n",
       "  39683    620062           90       980          316         -236       925   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  122875  1919937           50      1019          128         -800       935   \n",
       "  122876  1919953           50      1039          138         -818       935   \n",
       "  122877  1919968           50      1078          138         -800       935   \n",
       "  122878  1919984           60      1068           89         -809       935   \n",
       "  122879  1920000           50      1068          108         -827       935   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  39679           282            116         980            -58      0  \n",
       "  39680           272            145        1028            -87      0  \n",
       "  39681           242            135        1038           -126      0  \n",
       "  39682           202            135        1019            -87      0  \n",
       "  39683           242             67         971            -58      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  122875          -80             38         933           -174      0  \n",
       "  122876          232             38         923           -174      0  \n",
       "  122877          -70             38         933           -174      0  \n",
       "  122878          -70             38         942           -174      0  \n",
       "  122879          -80             38         923           -174      0  \n",
       "  \n",
       "  [49284 rows x 11 columns]],\n",
       " 9: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  26879    420000           50      1039          188         -209       962   \n",
       "  26880    420015           50      1039          168         -209       962   \n",
       "  26881    420031           50      1029          168         -209       972   \n",
       "  26882    420046           50      1029          168         -209       972   \n",
       "  26883    420062           50      1019          178         -209       972   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  163195  2549937          202      1019          188         -890       111   \n",
       "  163196  2549953          202      1009          178         -863       157   \n",
       "  163197  2549968          212      1000          158         -881       203   \n",
       "  163198  2549984          202      1019          148         -890       240   \n",
       "  163199  2550000          202      1019          178         -927       259   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  26879            90              9        1019             77      0  \n",
       "  26880           101             19        1009             67      0  \n",
       "  26881            80             29        1000             77      0  \n",
       "  26882            90             19        1009             58      0  \n",
       "  26883           111             19        1019            106      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  163195          515           -155         971            291      0  \n",
       "  163196          434           -155         980            281      0  \n",
       "  163197          373           -155         971            281      0  \n",
       "  163198          393           -155         961            252      0  \n",
       "  163199          434           -155         971            262      0  \n",
       "  \n",
       "  [111365 rows x 11 columns]],\n",
       " 10: [           time  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       "  38399    600000         -353       950          178          172       953   \n",
       "  38400    600015         -383       960          198          181       944   \n",
       "  38401    600031         -363       950          178          190       935   \n",
       "  38402    600046         -363       950          198          172       944   \n",
       "  38403    600062         -343       960          207          163       944   \n",
       "  ...         ...          ...       ...          ...          ...       ...   \n",
       "  185595  2899937         -505       823          336            0         0   \n",
       "  185596  2899953         -505       823          336            0         0   \n",
       "  185597  2899968         -505       823          336            0         0   \n",
       "  185598  2899984         -505       823          336            0         0   \n",
       "  185599  2900000         -505       823          336            0         0   \n",
       "  \n",
       "          leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  label  \n",
       "  38399           232             87        1000           -116      0  \n",
       "  38400           212             97         990            -87      0  \n",
       "  38401           232             97        1000           -116      0  \n",
       "  38402           202             97        1000            -97      0  \n",
       "  38403           232            106        1000            -97      0  \n",
       "  ...             ...            ...         ...            ...    ...  \n",
       "  185595            0              0           0              0      0  \n",
       "  185596            0              0           0              0      0  \n",
       "  185597            0              0           0              0      0  \n",
       "  185598            0              0           0              0      0  \n",
       "  185599            0              0           0              0      0  \n",
       "  \n",
       "  [142722 rows x 11 columns]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbb13a",
   "metadata": {
    "id": "a3dbb13a"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0f5a70",
   "metadata": {
    "id": "2d0f5a70"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 64 # Sample rate in herz\n",
    "STEP_SIZE = 3 # Step size in samples\n",
    "WINDOW_LENGTH = 192\n",
    "POWER_THRESHOLD = 2 ** 12\n",
    "# sensors = ['trunk_hor_fwd', 'trunk_vert', 'trunk_hor_lat']\n",
    "sensors = ['ank_hor_fwd', 'ank_vert', 'ank_hor_lat', 'leg_hor_fwd', 'leg_vert', 'leg_hor_lat', 'trunk_hor_fwd', 'trunk_vert', 'trunk_hor_lat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c3270",
   "metadata": {
    "id": "ec9c3270"
   },
   "source": [
    "Iterates through each sensor axis in a 4-second window and extract 256 samples with a 32 step increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d345d6a9",
   "metadata": {
    "id": "d345d6a9"
   },
   "outputs": [],
   "source": [
    "def integrate(x, SR):\n",
    "    return ( (sum(x[1:]) / SR) + (sum(x[:-1]) / SR) ) / 2\n",
    "\n",
    "def is_standing(y):\n",
    "    NFFT = WINDOW_LENGTH\n",
    "    locomotion_band = [0.5, 3]\n",
    "    freeze_band = [3, 8]\n",
    "\n",
    "    f_res = SAMPLE_RATE / NFFT\n",
    "    f_nr_LBs  = round(locomotion_band[0] / f_res) - 1 # -1 to match with baechlin's matlab functions (one indexed)\n",
    "    f_nr_LBe  = round(locomotion_band[1] / f_res)\n",
    "    f_nr_FBs  = round(freeze_band[0] / f_res) - 1\n",
    "    f_nr_FBe  = round(freeze_band[1] / f_res)\n",
    "\n",
    "    # Get signal inside window\n",
    "    y -= y.mean() # Make signal zero-mean, removing DC component\n",
    "\n",
    "    # Compute FFT and its real number equivalent\n",
    "    Y = fft(y, NFFT)\n",
    "    Pyy = Y * Y.conjugate()\n",
    "\n",
    "    # Area Under Curve (AUC) of the locomotion and freeze band\n",
    "    area_loco_band = integrate(Pyy[f_nr_LBs:f_nr_LBe].real / NFFT, SAMPLE_RATE)\n",
    "    area_freeze_band = integrate(Pyy[f_nr_FBs:f_nr_FBe].real / NFFT, SAMPLE_RATE)\n",
    "\n",
    "    return (area_loco_band + area_freeze_band) < POWER_THRESHOLD\n",
    "\n",
    "def extract_features(data, labels):\n",
    "    window_num = int(np.floor((len(data) - WINDOW_LENGTH + STEP_SIZE) / STEP_SIZE))\n",
    "\n",
    "    # initialize feature dict\n",
    "    windowed_data = np.full((window_num, WINDOW_LENGTH, len(sensors)), np.nan)\n",
    "    window_labels = np.full(window_num, np.nan)\n",
    "\n",
    "    current_pos = WINDOW_LENGTH;\n",
    "    for i in range(window_num):\n",
    "        start = current_pos - WINDOW_LENGTH\n",
    "        window = data[start:current_pos]\n",
    "\n",
    "        # uses trunk horizontal axis to check if subject is standing or actually freezing (Moore's idea) \n",
    "        #     if not is_standing(window['trunk_hor_fwd']):\n",
    "        windowed_data[i] = window\n",
    "        window_labels[i] = labels[start:current_pos].value_counts().index.values[0].astype(int)\n",
    "        current_pos += STEP_SIZE\n",
    "  \n",
    "  return windowed_data, window_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0f130b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d0f130b",
    "outputId": "a709c3cd-200b-4084-c143-c4b1f5659feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 1\n",
      " Session 0\n",
      " Session 1\n",
      "Patient 2\n",
      " Session 0\n",
      " Session 1\n",
      "Patient 3\n",
      " Session 0\n",
      " Session 1\n",
      " Session 2\n",
      "Patient 5\n",
      " Session 0\n",
      " Session 1\n",
      "Patient 6\n",
      " Session 0\n",
      " Session 1\n",
      "Patient 7\n",
      " Session 0\n",
      " Session 1\n",
      "Patient 8\n",
      " Session 0\n",
      "Patient 9\n",
      " Session 0\n"
     ]
    }
   ],
   "source": [
    "def process_dataframes(dfs):\n",
    "    subjects_dfs = dict(zip(dataframes.keys(), [pd.DataFrame() for x in dataframes.keys()]))\n",
    "    # Iterate through dataset subjects, subject sessions and subject sensors' \n",
    "    # axis measurements in a session                    \n",
    "    for i, subject in dfs.items():\n",
    "        print(f'Patient {i}')\n",
    "\n",
    "        subject_df = np.empty((1,WINDOW_LENGTH, len(sensors)))\n",
    "        subject_df = np.delete(subject_df, (0), axis=0)\n",
    "        subject_y = []\n",
    "\n",
    "        for j, session_data in enumerate(subject):\n",
    "            sensor_data = session_data.loc[:, sensors]\n",
    "            print(f' Session {j}')\n",
    "\n",
    "            session_windows, session_labels = extract_features(sensor_data, session_data['label'])\n",
    "            subject_df = np.vstack((subject_df, session_windows))\n",
    "            subject_y = np.concatenate((subject_y, session_labels), axis=0)\n",
    "\n",
    "        window_num = subject_df.shape[0]\n",
    "        subject_reshape = subject_df.reshape((window_num * WINDOW_LENGTH, len(sensors)))\n",
    "        subject_no_nan = subject_reshape[~np.isnan(subject_reshape).any(axis=1)]\n",
    "        clean_y = subject_y[~np.isnan(subject_y)]\n",
    "\n",
    "        label_index = pd.MultiIndex.from_product([clean_y, np.arange(WINDOW_LENGTH)], names=['label','time_step'])\n",
    "        subjects_dfs[i] = pd.DataFrame(subject_no_nan, columns=sensors, index=label_index)\n",
    "    return subjects_dfs\n",
    "\n",
    "windowed_dataframes = process_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4318f742",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4318f742",
    "outputId": "d0b02066-3d72-43e7-885c-25ede2894ac1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                -50.0     960.0        316.0       -127.0     935.0   \n",
       "       1               -111.0     970.0        336.0       -127.0     953.0   \n",
       "       2                -90.0    1019.0        326.0       -154.0     972.0   \n",
       "       3                -70.0    1000.0        316.0       -145.0     962.0   \n",
       "       4                -70.0     980.0        336.0       -109.0     972.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187             -202.0    1009.0        148.0        754.0     490.0   \n",
       "       188             -212.0    1039.0        148.0        763.0     509.0   \n",
       "       189             -212.0    1029.0        138.0        772.0     518.0   \n",
       "       190             -222.0    1009.0        148.0        772.0     500.0   \n",
       "       191             -212.0     990.0        168.0        763.0     500.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                202.0          -29.0      1028.0            0.0  \n",
       "       1                212.0          -19.0      1028.0          -29.0  \n",
       "       2                202.0          -19.0      1000.0           -9.0  \n",
       "       3                171.0          -29.0      1019.0          -19.0  \n",
       "       4                171.0          -48.0      1019.0           -9.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187             -252.0          495.0       742.0          456.0  \n",
       "       188             -262.0          485.0       742.0          466.0  \n",
       "       189             -262.0          485.0       733.0          456.0  \n",
       "       190             -222.0          495.0       752.0          456.0  \n",
       "       191             -171.0          485.0       761.0          436.0  \n",
       " \n",
       " [3744960 rows x 9 columns],\n",
       " 2:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0               -181.0    1000.0        267.0         36.0    1000.0   \n",
       "       1               -181.0     990.0        267.0         27.0     981.0   \n",
       "       2               -181.0     990.0        267.0         36.0     972.0   \n",
       "       3               -181.0     990.0        267.0         27.0     990.0   \n",
       "       4               -171.0     990.0        277.0         27.0    1000.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187             -151.0     882.0        485.0        881.0     250.0   \n",
       "       188             -151.0     882.0        485.0        936.0     305.0   \n",
       "       189             -161.0     901.0        485.0        954.0     333.0   \n",
       "       190             -161.0     901.0        485.0        954.0     342.0   \n",
       "       191             -151.0     901.0        495.0        881.0     342.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                 80.0          -67.0      1000.0          -19.0  \n",
       "       1                 80.0          -67.0      1009.0           -9.0  \n",
       "       2                 90.0          -58.0      1000.0           -9.0  \n",
       "       3                 90.0          -58.0      1019.0           -9.0  \n",
       "       4                 80.0          -48.0      1019.0           -9.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187              353.0           38.0       961.0          349.0  \n",
       "       188              424.0           19.0       961.0          330.0  \n",
       "       189              393.0           48.0       952.0          368.0  \n",
       "       190              333.0           29.0       952.0          359.0  \n",
       "       191              373.0           38.0       961.0          359.0  \n",
       " \n",
       " [2989440 rows x 9 columns],\n",
       " 3:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                -70.0    1000.0        287.0       -236.0     944.0   \n",
       "       1                -70.0    1009.0        277.0       -254.0     962.0   \n",
       "       2                -90.0    1009.0        277.0       -254.0     944.0   \n",
       "       3                -80.0     990.0        257.0       -254.0     962.0   \n",
       "       4                -90.0    1009.0        277.0       -245.0     972.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187               50.0    1029.0        188.0          0.0       0.0   \n",
       "       188               60.0    1029.0        168.0          0.0       0.0   \n",
       "       189               80.0    1019.0        178.0          0.0       0.0   \n",
       "       190               70.0    1019.0        188.0          0.0       0.0   \n",
       "       191               70.0    1019.0        188.0          0.0       0.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                 20.0         -116.0      1009.0          -67.0  \n",
       "       1                 30.0         -106.0      1009.0          -97.0  \n",
       "       2                 40.0         -106.0       990.0          -67.0  \n",
       "       3                 40.0         -106.0      1019.0          -97.0  \n",
       "       4                 10.0         -106.0      1009.0          -87.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187                0.0           48.0       961.0         -203.0  \n",
       "       188                0.0           48.0       961.0         -203.0  \n",
       "       189                0.0          514.0       914.0          -67.0  \n",
       "       190                0.0         -165.0       933.0          203.0  \n",
       "       191                0.0           67.0      1000.0         -155.0  \n",
       " \n",
       " [3004224 rows x 9 columns],\n",
       " 5:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0               -111.0    1019.0        247.0        109.0     981.0   \n",
       "       1               -101.0    1009.0        247.0        100.0     962.0   \n",
       "       2               -121.0    1000.0        247.0        109.0     962.0   \n",
       "       3                -90.0    1000.0        247.0         81.0     972.0   \n",
       "       4               -111.0     990.0        267.0        100.0     972.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187              202.0    1009.0          9.0        781.0     537.0   \n",
       "       188              212.0    1000.0          9.0        800.0     518.0   \n",
       "       189              202.0    1029.0         19.0        781.0     509.0   \n",
       "       190              191.0    1019.0          0.0        781.0     527.0   \n",
       "       191              202.0    1019.0          9.0        790.0     518.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                 90.0          -97.0      1009.0          -87.0  \n",
       "       1                121.0         -106.0      1000.0          -87.0  \n",
       "       2                 90.0         -116.0      1019.0          -97.0  \n",
       "       3                 80.0          -97.0      1009.0          -48.0  \n",
       "       4                101.0          -97.0      1019.0          -77.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187             -121.0          825.0       466.0          262.0  \n",
       "       188             -111.0          815.0       466.0          281.0  \n",
       "       189             -121.0          844.0       466.0          281.0  \n",
       "       190             -121.0          834.0       457.0          271.0  \n",
       "       191             -141.0          805.0       457.0          291.0  \n",
       " \n",
       " [4111680 rows x 9 columns],\n",
       " 6:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                101.0    1019.0        198.0       -136.0     953.0   \n",
       "       1                121.0    1009.0        217.0       -127.0     953.0   \n",
       "       2                101.0    1000.0        198.0       -145.0     953.0   \n",
       "       3                111.0    1009.0        227.0       -127.0     944.0   \n",
       "       4                 90.0    1019.0        198.0       -136.0     953.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187               80.0    1088.0        108.0         63.0     861.0   \n",
       "       188              161.0    1039.0         49.0        772.0     500.0   \n",
       "       189               60.0    1009.0         99.0      -1536.0    1564.0   \n",
       "       190               60.0    1019.0        118.0       -681.0     500.0   \n",
       "       191               20.0    1088.0         79.0        -54.0     962.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                252.0          300.0       942.0         -165.0  \n",
       "       1                262.0          310.0       942.0         -184.0  \n",
       "       2                232.0          320.0       942.0         -194.0  \n",
       "       3                242.0          300.0       933.0         -184.0  \n",
       "       4                242.0          310.0       942.0         -194.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187               80.0          718.0       961.0          135.0  \n",
       "       188              767.0          776.0       961.0          194.0  \n",
       "       189              252.0          834.0       961.0          145.0  \n",
       "       190              252.0          854.0       961.0          223.0  \n",
       "       191              545.0          844.0       961.0          145.0  \n",
       " \n",
       " [3276672 rows x 9 columns],\n",
       " 7:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                 50.0    1019.0         19.0       -272.0     953.0   \n",
       "       1                 30.0    1039.0         19.0       -263.0     953.0   \n",
       "       2                 50.0    1029.0         19.0       -263.0     953.0   \n",
       "       3                 50.0    1029.0          9.0       -263.0     953.0   \n",
       "       4                 50.0    1029.0          0.0       -281.0     962.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187               30.0    1009.0        108.0       -854.0     472.0   \n",
       "       188               10.0    1039.0         99.0       -854.0     472.0   \n",
       "       189               20.0    1029.0        108.0       -845.0     462.0   \n",
       "       190               20.0    1049.0         89.0       -845.0     453.0   \n",
       "       191               20.0    1029.0        108.0       -845.0     453.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                -50.0          300.0       961.0          -19.0  \n",
       "       1                -50.0          291.0       961.0            0.0  \n",
       "       2                -40.0          300.0       971.0            0.0  \n",
       "       3                -50.0          300.0       971.0            9.0  \n",
       "       4                -70.0          310.0       952.0          -19.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187              131.0          -87.0       495.0          -29.0  \n",
       "       188              131.0          -87.0       466.0          -29.0  \n",
       "       189              141.0          -97.0       485.0          -29.0  \n",
       "       190              161.0          -97.0       495.0          -29.0  \n",
       "       191              131.0          -87.0       476.0          -29.0  \n",
       " \n",
       " [3706560 rows x 9 columns],\n",
       " 8:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                101.0    1009.0        306.0       -163.0    1000.0   \n",
       "       1                 60.0    1009.0        316.0       -290.0     925.0   \n",
       "       2                 40.0    1000.0        297.0       -318.0     898.0   \n",
       "       3                 60.0    1019.0        287.0       -254.0     962.0   \n",
       "       4                 90.0     980.0        316.0       -236.0     925.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187               50.0    1019.0        128.0       -800.0     935.0   \n",
       "       188               50.0    1039.0        138.0       -818.0     935.0   \n",
       "       189               50.0    1078.0        138.0       -800.0     935.0   \n",
       "       190               60.0    1068.0         89.0       -809.0     935.0   \n",
       "       191               50.0    1068.0        108.0       -827.0     935.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                282.0          116.0       980.0          -58.0  \n",
       "       1                272.0          145.0      1028.0          -87.0  \n",
       "       2                242.0          135.0      1038.0         -126.0  \n",
       "       3                202.0          135.0      1019.0          -87.0  \n",
       "       4                242.0           67.0       971.0          -58.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187              -80.0           38.0       933.0         -174.0  \n",
       "       188              232.0           38.0       923.0         -174.0  \n",
       "       189              -70.0           38.0       933.0         -174.0  \n",
       "       190              -70.0           38.0       942.0         -174.0  \n",
       "       191              -80.0           38.0       923.0         -174.0  \n",
       " \n",
       " [1686336 rows x 9 columns],\n",
       " 9:                  ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  leg_vert  \\\n",
       " label time_step                                                              \n",
       " 0.0   0                 60.0    1019.0        188.0       -209.0     962.0   \n",
       "       1                 60.0    1000.0        178.0       -209.0     972.0   \n",
       "       2                 60.0    1019.0        158.0       -200.0     981.0   \n",
       "       3                 60.0    1019.0        158.0       -209.0     972.0   \n",
       "       4                 40.0    1029.0        178.0       -227.0     953.0   \n",
       " ...                      ...       ...          ...          ...       ...   \n",
       "       187              191.0    1009.0        158.0       -645.0     453.0   \n",
       "       188              191.0    1000.0        158.0       -636.0     462.0   \n",
       "       189              191.0    1000.0        168.0       -627.0     444.0   \n",
       "       190              202.0    1000.0        168.0       -636.0     462.0   \n",
       "       191              202.0    1019.0        168.0       -645.0     453.0   \n",
       " \n",
       "                  leg_hor_lat  trunk_hor_fwd  trunk_vert  trunk_hor_lat  \n",
       " label time_step                                                         \n",
       " 0.0   0                 90.0           38.0      1019.0           77.0  \n",
       "       1                 90.0           19.0      1009.0           87.0  \n",
       "       2                 80.0           29.0      1009.0           87.0  \n",
       "       3                101.0           29.0      1019.0           77.0  \n",
       "       4                111.0           29.0      1009.0           87.0  \n",
       " ...                      ...            ...         ...            ...  \n",
       "       187              595.0         -145.0       980.0          271.0  \n",
       "       188              606.0         -155.0       980.0          271.0  \n",
       "       189              616.0         -145.0       980.0          281.0  \n",
       "       190              616.0         -155.0       980.0          271.0  \n",
       "       191              616.0         -155.0       980.0          291.0  \n",
       " \n",
       " [2602752 rows x 9 columns]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowed_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1c27d8",
   "metadata": {
    "id": "de1c27d8"
   },
   "outputs": [],
   "source": [
    "freq_df = pd.concat(windowed_dataframes, names=['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4c4be7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ank_hor_fwd</th>\n",
       "      <th>ank_vert</th>\n",
       "      <th>ank_hor_lat</th>\n",
       "      <th>leg_hor_fwd</th>\n",
       "      <th>leg_vert</th>\n",
       "      <th>leg_hor_lat</th>\n",
       "      <th>trunk_hor_fwd</th>\n",
       "      <th>trunk_vert</th>\n",
       "      <th>trunk_hor_lat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>time_step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>-50.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-111.0</td>\n",
       "      <td>970.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>-29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-70.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-70.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>187</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>-645.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>-636.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-627.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>202.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-636.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>202.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-645.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>291.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25122624 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  \\\n",
       "subject label time_step                                                    \n",
       "1       0.0   0                -50.0     960.0        316.0       -127.0   \n",
       "              1               -111.0     970.0        336.0       -127.0   \n",
       "              2                -90.0    1019.0        326.0       -154.0   \n",
       "              3                -70.0    1000.0        316.0       -145.0   \n",
       "              4                -70.0     980.0        336.0       -109.0   \n",
       "...                              ...       ...          ...          ...   \n",
       "9       0.0   187              191.0    1009.0        158.0       -645.0   \n",
       "              188              191.0    1000.0        158.0       -636.0   \n",
       "              189              191.0    1000.0        168.0       -627.0   \n",
       "              190              202.0    1000.0        168.0       -636.0   \n",
       "              191              202.0    1019.0        168.0       -645.0   \n",
       "\n",
       "                         leg_vert  leg_hor_lat  trunk_hor_fwd  trunk_vert  \\\n",
       "subject label time_step                                                     \n",
       "1       0.0   0             935.0        202.0          -29.0      1028.0   \n",
       "              1             953.0        212.0          -19.0      1028.0   \n",
       "              2             972.0        202.0          -19.0      1000.0   \n",
       "              3             962.0        171.0          -29.0      1019.0   \n",
       "              4             972.0        171.0          -48.0      1019.0   \n",
       "...                           ...          ...            ...         ...   \n",
       "9       0.0   187           453.0        595.0         -145.0       980.0   \n",
       "              188           462.0        606.0         -155.0       980.0   \n",
       "              189           444.0        616.0         -145.0       980.0   \n",
       "              190           462.0        616.0         -155.0       980.0   \n",
       "              191           453.0        616.0         -155.0       980.0   \n",
       "\n",
       "                         trunk_hor_lat  \n",
       "subject label time_step                 \n",
       "1       0.0   0                    0.0  \n",
       "              1                  -29.0  \n",
       "              2                   -9.0  \n",
       "              3                  -19.0  \n",
       "              4                   -9.0  \n",
       "...                                ...  \n",
       "9       0.0   187                271.0  \n",
       "              188                271.0  \n",
       "              189                281.0  \n",
       "              190                271.0  \n",
       "              191                291.0  \n",
       "\n",
       "[25122624 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215ab1f",
   "metadata": {
    "id": "b215ab1f"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c66e77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34c66e77",
    "outputId": "009eef5b-839c-4674-84b2-2a0c936bb6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 6, 7, 9] [8]\n",
      "(122064, 192, 9) (8783, 192, 9)\n",
      "(122064,) (8783,)\n"
     ]
    }
   ],
   "source": [
    "subjects = set(windowed_dataframes.keys())\n",
    "\n",
    "TEST_NUM = 1\n",
    "\n",
    "test_subjects = set(rng.choice(list(windowed_dataframes.keys()), TEST_NUM))\n",
    "while len(test_subjects) != TEST_NUM:\n",
    "    test_subjects = set(rng.choice(list(windowed_dataframes.keys()), TEST_NUM))\n",
    "train_subjects = subjects - test_subjects\n",
    "\n",
    "# least unbalanced subject goes to testing\n",
    "# train_subjects = [1,2,3,5,6,7,8]\n",
    "# test_subjects = [9]\n",
    "train_subjects = [1,2,3,5,6,7,9]\n",
    "test_subjects = [8]\n",
    "\n",
    "print(train_subjects, test_subjects)\n",
    "\n",
    "index_frame = freq_df.index.to_frame()\n",
    "\n",
    "train_df = freq_df[index_frame['subject'].isin(train_subjects)]\n",
    "test_df = freq_df[index_frame['subject'].isin(test_subjects)]\n",
    "\n",
    "X_train = train_df.values.reshape((int(train_df.shape[0] / WINDOW_LENGTH), WINDOW_LENGTH, train_df.shape[1]))\n",
    "X_test = test_df.values.reshape((int(test_df.shape[0] / WINDOW_LENGTH), WINDOW_LENGTH, test_df.shape[1]))\n",
    "\n",
    "y_train = pd.Series(train_df.index.get_level_values(1).values.astype(int))\n",
    "y_train = y_train[range(0, y_train.shape[0], WINDOW_LENGTH)]\n",
    "y_test = pd.Series(test_df.index.get_level_values(1).values.astype(int))\n",
    "y_test = y_test[range(0, y_test.shape[0], WINDOW_LENGTH)]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a0fc46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7a0fc46",
    "outputId": "e8b78be5-21ca-41b9-82ee-244e513e83ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6882\n",
       "1    1901\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c708e0",
   "metadata": {
    "id": "23c708e0"
   },
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d79967c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99972\n",
       "1    22092\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5d89ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ab5d89ea",
    "outputId": "b3c9b05f-74f9-455f-8c93-0e5f5e1b6991",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_train.value_counts()).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b647f4c",
   "metadata": {
    "id": "7b647f4c"
   },
   "outputs": [],
   "source": [
    "def random_undersampling(X_train, y_train):\n",
    "    lesser_label_value = y_train.value_counts()[1]\n",
    "    y_zeros = y_train[y_train == 0]\n",
    "    y_ones = y_train[y_train == 1]\n",
    "    y_zeros_choice = rng.choice(y_train.index, lesser_label_value)\n",
    "    \n",
    "    y_train_zeros = np.zeros(lesser_label_value)\n",
    "    X_train_zeros = X_train[np.floor(y_zeros_choice / WINDOW_LENGTH).astype(int), :, :]\n",
    "    y_train_ones = np.ones(lesser_label_value)\n",
    "    X_train_ones = X_train[np.floor(y_ones.index / WINDOW_LENGTH).astype(int), :, :]\n",
    "\n",
    "    y_train_under = np.vstack((y_train_zeros.reshape(-1, 1), y_train_ones.reshape(-1, 1)))\n",
    "    X_train_under = np.vstack([X_train_zeros, X_train_ones])\n",
    "    X_train_under, y_train_under = shuffle(X_train_under, y_train_under, random_state=LE_MAGIC_NUM)\n",
    "    y_train_under = pd.Series(y_train_under.reshape(-1))\n",
    "\n",
    "    return X_train_under, y_train_under\n",
    "\n",
    "X_train_under, y_train_under = random_undersampling(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f985c08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "4f985c08",
    "outputId": "1b6c548e-3e30-4471-b0df-b13543afe7f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_train_under.value_counts()).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89017dd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89017dd9",
    "outputId": "e28a895a-4f71-4cdd-abbe-4a7a1b0b55aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44184, 192, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_under.shape\n",
    "# y_train_under.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47b2c4",
   "metadata": {
    "id": "fb47b2c4"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "643b88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Concatenate, Activation, Add, GlobalAveragePooling1D, \\\n",
    "    Dense, LSTM, TimeDistributed, Reshape, BatchNormalization, Bidirectional, Flatten, MaxPooling1D, Dropout, \\\n",
    "    SeparableConv1D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Reduction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91a0585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ispl_inception(x_shape,\n",
    "                   n_classes,\n",
    "                   filters_number,\n",
    "                   network_depth=5,\n",
    "                   use_residual=True,\n",
    "                   use_bottleneck=True,\n",
    "                   max_kernel_size=20,\n",
    "                   learning_rate=0.01,\n",
    "                   bottleneck_size=32,\n",
    "                   regularization_rate=0.01,\n",
    "                   metrics=['accuracy']):\n",
    "    dim_length = x_shape[1]  # number of samples in a time series\n",
    "    dim_channels = x_shape[2]  # number of channels\n",
    "    weightinit = 'lecun_uniform'  # weight initialization\n",
    "\n",
    "    def inception_module(input_tensor, stride=1, activation='relu'):\n",
    "\n",
    "        # The  channel number is greater than 1\n",
    "        if use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = Conv1D(filters=bottleneck_size,\n",
    "                                     kernel_size=1,\n",
    "                                     padding='same',\n",
    "                                     activation=activation,\n",
    "                                     kernel_initializer=weightinit,\n",
    "\n",
    "                                     use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        kernel_sizes = [max_kernel_size // (2 ** i) for i in range(3)]\n",
    "        conv_list = []\n",
    "\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv_list.append(Conv1D(filters=filters_number,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    strides=stride,\n",
    "                                    padding='same',\n",
    "                                    activation=activation,\n",
    "                                    kernel_initializer=weightinit,\n",
    "                                    kernel_regularizer=l2(regularization_rate),\n",
    "                                    use_bias=False)(input_inception))\n",
    "\n",
    "        max_pool_1 = MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_last = Conv1D(filters=filters_number,\n",
    "                           kernel_size=1,\n",
    "                           padding='same',\n",
    "                           activation=activation,\n",
    "                           kernel_initializer=weightinit,\n",
    "                           kernel_regularizer=l2(regularization_rate),\n",
    "                           use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_last)\n",
    "\n",
    "        x = Concatenate(axis=2)(conv_list)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def shortcut_layer(input_tensor, out_tensor):\n",
    "        shortcut_y = Conv1D(filters=int(out_tensor.shape[-1]),\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            kernel_initializer=weightinit,\n",
    "                            kernel_regularizer=l2(regularization_rate),\n",
    "                            use_bias=False)(input_tensor)\n",
    "        shortcut_y = BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = Add()([shortcut_y, out_tensor])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    # Build the actual model:\n",
    "    input_layer = Input((dim_length, dim_channels))\n",
    "    x = BatchNormalization()(input_layer)  # Added batchnorm (not in original paper)\n",
    "    input_res = x\n",
    "\n",
    "    for depth in range(network_depth):\n",
    "        x = inception_module(x)\n",
    "\n",
    "        if use_residual and depth % 3 == 2:\n",
    "            x = shortcut_layer(input_res, x)\n",
    "            input_res = x\n",
    "\n",
    "    gap_layer = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Final classification layer\n",
    "    output_layer = Dense(n_classes, activation='sigmoid',\n",
    "                         kernel_initializer=weightinit, kernel_regularizer=l2(regularization_rate))(gap_layer)\n",
    "\n",
    "    # Create model and compile\n",
    "    m = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    m.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate, amsgrad=True),\n",
    "              metrics=metrics)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c786d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 192, 9)]     0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 192, 9)      36          ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 192, 32)      288         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 192, 9)       0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 192, 64)      139264      ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 192, 64)      69632       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 192, 64)      34816       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 192, 64)      576         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 192, 256)     0           ['conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 192, 256)    1024        ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 192, 256)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 192, 32)      8192        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 192, 256)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 192, 64)      139264      ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 192, 64)      69632       ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 192, 64)      34816       ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 192, 64)      16384       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192, 256)     0           ['conv1d_6[0][0]',               \n",
      "                                                                  'conv1d_7[0][0]',               \n",
      "                                                                  'conv1d_8[0][0]',               \n",
      "                                                                  'conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 192, 256)    1024        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 192, 256)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 192, 32)      8192        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 192, 256)    0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 192, 64)      139264      ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 192, 64)      69632       ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 192, 64)      34816       ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 192, 64)      16384       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 192, 256)     0           ['conv1d_11[0][0]',              \n",
      "                                                                  'conv1d_12[0][0]',              \n",
      "                                                                  'conv1d_13[0][0]',              \n",
      "                                                                  'conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 192, 256)     2304        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 192, 256)    1024        ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 192, 256)    1024        ['conv1d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 192, 256)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 192, 256)     0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 192, 256)     0           ['add[0][0]']                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 192, 32)      8192        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 192, 256)    0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 192, 64)      139264      ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 192, 64)      69632       ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 192, 64)      34816       ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 192, 64)      16384       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 192, 256)     0           ['conv1d_17[0][0]',              \n",
      "                                                                  'conv1d_18[0][0]',              \n",
      "                                                                  'conv1d_19[0][0]',              \n",
      "                                                                  'conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 192, 256)    1024        ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 192, 256)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 192, 32)      8192        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 192, 256)    0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 192, 64)      139264      ['conv1d_21[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 192, 64)      69632       ['conv1d_21[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 192, 64)      34816       ['conv1d_21[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 192, 64)      16384       ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 192, 256)     0           ['conv1d_22[0][0]',              \n",
      "                                                                  'conv1d_23[0][0]',              \n",
      "                                                                  'conv1d_24[0][0]',              \n",
      "                                                                  'conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 192, 256)    1024        ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 192, 256)     0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['activation_5[0][0]']           \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            257         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,326,469\n",
      "Trainable params: 1,323,379\n",
      "Non-trainable params: 3,090\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 350\n",
    "patience = 300\n",
    "use_residual = True\n",
    "use_bottleneck = True\n",
    "n_signals = 9  # Ankle acc - x,y,z; Upper Leg acc - x,y,z; Trunk acc - x,y,z;\n",
    "win_size = 192  # 1 sec -> 64 | 2.56 (3) sec -> 192 | 5 sec -> 320 # sampling rate = 64hz\n",
    "n_classes = 2  # 1 - no freeze (walk, stand, turn); 2 - freeze\n",
    "n_steps = 3  # Since we are using 3 seconds and 192 is divisible by 3\n",
    "length = 64  # Split each window of 192 time steps into sub sequences for the cnn\n",
    "    \n",
    "hyperparameters = {'learning_rate': 0.0005, 'regularization_rate': 0.00593,\n",
    "                   'network_depth': 5, 'filters_number': 64, 'max_kernel_size': 68,\n",
    "                   'use_residual': use_residual, 'use_bottleneck': use_bottleneck}\n",
    "mod_name = 'iSPLInception'\n",
    "\n",
    "model = ispl_inception(X_train.shape, 1, **hyperparameters)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "582273b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(_model, _X_train, _y_train, _X_test, _y_test, _epochs=20, patience=10,\n",
    "                   batch_size=64, _save_name='models/resnet', _log_dir='logs/fit'):\n",
    "    \"\"\"\n",
    "    Returns the best trained model and history objects of the currently provided train & test set\n",
    "    \"\"\"\n",
    "    early_stopping_monitor = EarlyStopping(patience=patience)\n",
    "\n",
    "    checkpoint_path = _save_name\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create checkpoint callback\n",
    "    cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                                  monitor='val_loss',\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=False,\n",
    "                                  verbose=0)\n",
    "    # Tensorboard Callback\n",
    "    tensorboard_callback = TensorBoard(log_dir=_log_dir, histogram_freq=1)\n",
    "\n",
    "    # Reduce Learning rate after plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=10,\n",
    "                                  min_lr=0.0001, verbose=1)\n",
    "\n",
    "    # Training the model\n",
    "    history = _model.fit(_X_train,\n",
    "                         _y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         validation_data=(_X_test, _y_test),\n",
    "                         epochs=_epochs,\n",
    "                         verbose=1,\n",
    "                         # shuffle=True,\n",
    "                         use_multiprocessing=True,\n",
    "                         callbacks=[tensorboard_callback, early_stopping_monitor, reduce_lr])\n",
    "    best_model = load_model(checkpoint_path)\n",
    "    return best_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4e39cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "1908/1908 [==============================] - 126s 58ms/step - loss: 0.9779 - accuracy: 0.9315 - val_loss: 0.7123 - val_accuracy: 0.7413 - lr: 5.0000e-04\n",
      "Epoch 2/350\n",
      " 380/1908 [====>.........................] - ETA: 1:25 - loss: 0.2281 - accuracy: 0.9598"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plot_history(history)\n\u001b[0;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [26], line 25\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(_model, _X_train, _y_train, _X_test, _y_test, _epochs, patience, batch_size, _save_name, _log_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     22\u001b[0m                               min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_X_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m_y_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_X_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# shuffle=True,\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m                     \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_monitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m load_model(checkpoint_path)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model, history\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = evaluate_model(model, X_train, y_train, X_test, y_test, patience=patience, _epochs=epochs)\n",
    "plot_history(history)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a0085c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing defs: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m mod \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/ispl_inception.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m mod\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\h5py\\__init__.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mhdf5_version_tuple \u001b[38;5;241m!=\u001b[39m version\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple:\n\u001b[0;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[0;32m     40\u001b[0m     ))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freezing\\lib\\site-packages\\h5py\\version.py:15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Versioning module for h5py.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m h5 \u001b[38;5;28;01mas\u001b[39;00m _h5\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\h5.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing defs: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "mod = load_model('models/ispl_inception.h5')\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6bcd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9c6bcd5",
    "outputId": "e3631329-c229-4038-adf4-7c09d8146381",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(X, summary):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = models.Sequential()\n",
    "    n_timesteps, n_features = X.shape[1], X.shape[2]\n",
    "\n",
    "    # Section 1\n",
    "    model.add(layers.Conv1D(filters=50, kernel_size=33, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "#                          kernel_initializer=tf.keras.initializers.he_uniform(seed=LE_MAGIC_NUM),\n",
    "#                          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "#                          activity_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=4, strides=2))\n",
    "\n",
    "    # Section 2\n",
    "    model.add(layers.Conv1D(filters=40, kernel_size=36, activation='relu',))\n",
    "#                          kernel_initializer=tf.keras.initializers.he_uniform(seed=LE_MAGIC_NUM),\n",
    "#                          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "#                          activity_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=4, strides=2))\n",
    "\n",
    "    # Section 3\n",
    "    model.add(layers.Conv1D(filters=20, kernel_size=37, activation='relu'))\n",
    "#                          kernel_initializer=tf.keras.initializers.he_uniform(seed=LE_MAGIC_NUM),\n",
    "#                          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "#                          activity_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=1, strides=2))\n",
    "\n",
    "    # Section 4\n",
    "#     model.add(layers.Conv1D(filters=100, kernel_size=1))\n",
    "    model.add(layers.Flatten())\n",
    "#     model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))\n",
    "\n",
    "    # Section 5\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train_under, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc2cfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dacc2cfd",
    "outputId": "c3e48360-42ff-4568-af04-b16bc9f59064"
   },
   "outputs": [],
   "source": [
    "model = build_model(X_train_under, False)\n",
    "\n",
    "def run_training(X, y, X_test, y_test, model, plot=False, verbose=0):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=70, min_delta=0.005, restore_best_weights=False)\n",
    "    history = model.fit(X, y, validation_data=(X_test, y_test), epochs=200, verbose=verbose, callbacks=[callback])\n",
    "    if plot:\n",
    "        plot_history(history)\n",
    "\n",
    "run_training(X_train_under, y_train_under, X_test, y_test, model, plot=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618994b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "618994b8",
    "outputId": "234f64c0-3fe6-42c3-a29e-531eb950003b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 1s 2ms/step\n",
      "(10255,) (10255,)\n",
      "Sensitivity: 0.3652246256239601, Specificity: 0.8768309769456121\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(y_pred.shape, y_test.shape)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "draw_confusion_matrix(cf_matrix)\n",
    "# Sensitivity: 0.5632279534109818, Specificity: 0.7223283658132722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "429bb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semi_supervised5672\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semi_supervised5672\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/semi_supervised5672')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXhSAGijvZDY",
   "metadata": {
    "id": "EXhSAGijvZDY"
   },
   "source": [
    "\n",
    "# Self-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24161d4",
   "metadata": {
    "id": "c24161d4"
   },
   "source": [
    "In order to prevent some of the common pitfalls of self-labeling and self-training like wrong estimations with high confidence, we'll implement an algorithm called \"Batch Learning\" (Mikos, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca6264",
   "metadata": {
    "id": "54ca6264"
   },
   "source": [
    "## Representative Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11571f87",
   "metadata": {
    "id": "11571f87"
   },
   "source": [
    "First, we have to get representative samples from our dataset. This is obtained by calculating the euclidean distance between our dataset and its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8f6b4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137745, 256, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = freq_df.values.reshape((int(freq_df.shape[0] / WINDOW_LENGTH), WINDOW_LENGTH, freq_df.shape[1]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bd9e4a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bd9e4a9",
    "outputId": "a2601a2a-8321-457d-cdb6-036efefeb73f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111377, 111376,   6761, ...,  24265,  24253,  24264], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates euclidean distance for each columns in dataset, then sorts them to obtain the sample with smallest distance\n",
    "distances = abs(X-X.mean(axis=0))\n",
    "\n",
    "sorted_index = distances.sum(axis=1).sum(axis=1).argsort()\n",
    "sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa7740",
   "metadata": {
    "id": "05fa7740"
   },
   "source": [
    "We collect a representative positive and negative sample, then calculate the euclidean distance between them and divide by 4 to create a \"safe zone\". During learning, any samples that falls inside a label's safe zone that has an opposite label is discarded. This mitigates the risk of the model deviating too much from the offline learning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649a7a1",
   "metadata": {
    "id": "e649a7a1"
   },
   "source": [
    "![title](images/safe_zone.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "397fdd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = freq_df.index.get_level_values('label').values.astype(int)\n",
    "y_all = y_all[range(0, y_all.shape[0], WINDOW_LENGTH)]\n",
    "y_sorted = y_all[sorted_index]\n",
    "y_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b79d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sorted = X[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac775616",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "ac775616",
    "outputId": "b78f0743-98c7-45b6-eefa-3dbe6f0e1b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 9) (1, 256, 9)\n"
     ]
    }
   ],
   "source": [
    "neg_index = np.argwhere(y_sorted == 0)\n",
    "pos_index = np.argwhere(y_sorted == 1)\n",
    "\n",
    "X_sorted_neg = X_sorted[neg_index]\n",
    "X_sorted_pos = X_sorted[pos_index]\n",
    "\n",
    "rep_neg_sample = X_sorted_neg[0]\n",
    "rep_pos_sample = X_sorted_pos[0]\n",
    "\n",
    "print(rep_neg_sample.shape, rep_pos_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69b1a4d3",
   "metadata": {
    "id": "69b1a4d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 25.25,  61.25,  76.75, ...,  17.  ,   0.  ,  14.75],\n",
       "        [111.  ,   7.5 ,  52.  , ...,   7.25,  31.  ,  29.25],\n",
       "        [ 47.75,  14.75,  42.  , ...,  29.25,  50.  ,  31.5 ],\n",
       "        ...,\n",
       "        [ 25.25,  24.5 ,   5.  , ...,   7.25,   0.  ,  12.25],\n",
       "        [ 58.  ,  14.75,  62.  , ...,   9.75,   9.75,  22.  ],\n",
       "        [ 70.75,  10.  ,  32.25, ...,  12.25,   2.5 ,  14.5 ]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_zone = abs(rep_neg_sample - rep_pos_sample) / 4\n",
    "safe_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae04f8",
   "metadata": {
    "id": "b9ae04f8"
   },
   "source": [
    "I'll also collect 5 representative samples which will be used in the batch learning phase so that the learning process isn't completely unsupervised  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "453a723d",
   "metadata": {
    "id": "453a723d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256, 9) (5, 256, 9)\n"
     ]
    }
   ],
   "source": [
    "X_offline_neg = X_sorted_neg[:5].reshape(-1, WINDOW_LENGTH, len(sensors))\n",
    "X_offline_pos = X_sorted_pos[:5].reshape(-1, WINDOW_LENGTH, len(sensors))\n",
    "\n",
    "# X_stack = np.vstack((rep_neg_samples, rep_pos_samples))\n",
    "# y_stack = np.vstack((y_sorted[neg_index][:5], y_sorted[pos_index][:5]))\n",
    "# X_rep, y_rep = shuffle(X_stack, y_stack)\n",
    "\n",
    "# X_rep = X_rep.reshape(X_rep.shape[0], WINDOW_LENGTH, len(sensors))\n",
    "# y_rep = y_rep.reshape(-1)\n",
    "\n",
    "print(X_offline_neg.shape, X_offline_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744c34c",
   "metadata": {},
   "source": [
    "## Batch Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44e80c",
   "metadata": {},
   "source": [
    "![title](images/learning_batch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eb242d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7851\n",
       "1    2404\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84fc3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    8410\n",
       "1    1845\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.predict(X_test).argmax(axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28d8e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    8410\n",
      "1    1845\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_pred).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c4c5e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10255"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc041fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    1029\n",
      "1.0       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "num_samples = 10\n",
    "shape = int(np.floor(X_test.shape[0] / num_samples))\n",
    "\n",
    "for i in range(0, shape, num_samples):\n",
    "    samples = X_test[i:i+num_samples]\n",
    "    predictions = model.predict(samples, verbose=0)\n",
    "#     highest_pred = predictions.argmax()\n",
    "    y_pred = np.append(y_pred, predictions.argmax(axis=1))\n",
    "#     prediction = predictions[highest_pred] if highest_pred == 1 else 1 - predictions[highest_pred]\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "# print(i, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8952b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.3652246256239601, Specificity: 0.8768309769456121\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y = np.zeros(X_test.shape[0])\n",
    "\n",
    "for i, sample in enumerate(X_test):\n",
    "    predictions = model.predict(np.array([sample]), verbose=0)[0]\n",
    "    highest_pred = predictions.argmax()\n",
    "    \n",
    "    pred_y[i] = highest_pred\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, pred_y)\n",
    "draw_confusion_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dbb3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_online_neg = []\n",
    "X_online_pos = []\n",
    "\n",
    "# Deep copy of offline training model\n",
    "offline_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "y_pred = []\n",
    "# for i, sample in enumerate(X_test[:10]):\n",
    "#     predictions = offline_model.predict(np.array([sample]), verbose=0)[0]\n",
    "#     highest_pred = predictions.argmax()\n",
    "    \n",
    "#     prediction = predictions[highest_pred] if highest_pred == 1 else 1 - predictions[highest_pred]\n",
    "#     y_pred = np.append(y_pred, prediction)\n",
    "\n",
    "y_pred = np.zeros(10)\n",
    "\n",
    "for i, sample in enumerate(X_test[10:]):\n",
    "    predictions = offline_model.predict(np.array([sample]), verbose=0)[0]\n",
    "    highest_pred = predictions.argmax()\n",
    "#     print(highest_pred, end=' ')\n",
    "\n",
    "#     prediction = predictions[highest_pred] if highest_pred == 1 else 1 - predictions[highest_pred]\n",
    "    if predictions[highest_pred] == 1:\n",
    "        prediction = predictions[highest_pred]\n",
    "    else:\n",
    "        1 - predictions[highest_pred]\n",
    "    \n",
    "    pred_mean = np.array(y_pred[-3:]).mean()\n",
    "#     print(\"(%.2f %d)\" % (pred_mean.shape, y_test.iloc[i]))\n",
    "    \n",
    "#     if pred_mean > .55:\n",
    "#         print('sus sus amogus')\n",
    "#         y_pred = np.append(y_pred, prediction)\n",
    "#         X_online_pos.append(sample)\n",
    "#     elif pred_mean < .25:\n",
    "#         y_pred = np.append(y_pred, prediction)\n",
    "#         X_online_neg.append(sample)\n",
    "#     else:\n",
    "#         continue\n",
    "    \n",
    "    if len(X_online_pos) >= 5 and len(X_online_neg) >= 5:\n",
    "        print(\"HORA DE TREINAR!\")\n",
    "        X_stack = np.vstack((X_online_pos[-5:], X_offline_pos, X_online_neg[-5:], X_offline_neg))\n",
    "        y_stack = np.ravel((np.ones(10), np.zeros(10)))\n",
    "        X_batch, y_batch = shuffle(X_stack, y_stack)\n",
    "        print(X_batch.shape, y_batch.shape)\n",
    "        offline_model.fit(X_batch, y_batch, epochs=200, batch_size=20, verbose=0)\n",
    "        # Remove os ltimos 5 de cada array\n",
    "        X_online_pos = X_online_pos[:-5]\n",
    "        X_online_neg = X_online_neg[:-5]\n",
    "\n",
    "#     No d pra aplicar o Rep Sample, porque eu tirei a mdia da base inteira e no em batch 256. \n",
    "#     Preciso encontrar o batch 256 que  mdia, se  que isso faz sentido\n",
    "#     if highest_pred == 1 and np.all(sample > rep_pos_sample + safe_zone or sample < rep_pos_sample - safe_zone):\n",
    "#         break\n",
    "#     elif highest_pred == 0 and np.all(sample > rep_neg_sample + safe_zone or sample < rep_pos_sample - safe_zone):\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed3768",
   "metadata": {
    "id": "93ed3768"
   },
   "source": [
    "# 'Leave One Out' Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67f0b273",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "7c05d425",
    "outputId": "400dac58-159a-40d3-f9da-dbfe0b08ad81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ank_hor_fwd</th>\n",
       "      <th>ank_vert</th>\n",
       "      <th>ank_hor_lat</th>\n",
       "      <th>leg_hor_fwd</th>\n",
       "      <th>leg_vert</th>\n",
       "      <th>leg_hor_lat</th>\n",
       "      <th>trunk_hor_fwd</th>\n",
       "      <th>trunk_vert</th>\n",
       "      <th>trunk_hor_lat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>time_step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>-101.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>-136.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-70.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-50.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>-29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>251</th>\n",
       "      <td>191.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>-627.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>-636.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>171.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>-636.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-636.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>191.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-627.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32637440 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ank_hor_fwd  ank_vert  ank_hor_lat  leg_hor_fwd  \\\n",
       "subject label time_step                                                    \n",
       "1       0.0   0               -101.0     980.0        326.0       -136.0   \n",
       "              1                -80.0    1000.0        326.0       -100.0   \n",
       "              2                -70.0     990.0        336.0        -81.0   \n",
       "              3                -90.0     990.0        306.0        -81.0   \n",
       "              4                -50.0     990.0        316.0       -100.0   \n",
       "...                              ...       ...          ...          ...   \n",
       "9       0.0   251              191.0     990.0        158.0       -627.0   \n",
       "              252              191.0    1000.0        158.0       -636.0   \n",
       "              253              171.0    1000.0        178.0       -636.0   \n",
       "              254              191.0    1009.0        148.0       -636.0   \n",
       "              255              191.0    1009.0        148.0       -627.0   \n",
       "\n",
       "                         leg_vert  leg_hor_lat  trunk_hor_fwd  trunk_vert  \\\n",
       "subject label time_step                                                     \n",
       "1       0.0   0             972.0        181.0          -48.0      1019.0   \n",
       "              1             962.0        181.0          -58.0      1019.0   \n",
       "              2             972.0        212.0          -58.0      1009.0   \n",
       "              3             981.0        202.0          -58.0      1009.0   \n",
       "              4             962.0        202.0          -48.0      1028.0   \n",
       "...                           ...          ...            ...         ...   \n",
       "9       0.0   251           444.0        595.0         -155.0       961.0   \n",
       "              252           472.0        606.0         -145.0       980.0   \n",
       "              253           462.0        616.0         -135.0       980.0   \n",
       "              254           453.0        616.0         -145.0       971.0   \n",
       "              255           453.0        616.0         -145.0       980.0   \n",
       "\n",
       "                         trunk_hor_lat  \n",
       "subject label time_step                 \n",
       "1       0.0   0                   -9.0  \n",
       "              1                  -58.0  \n",
       "              2                    0.0  \n",
       "              3                    0.0  \n",
       "              4                  -29.0  \n",
       "...                                ...  \n",
       "9       0.0   251                281.0  \n",
       "              252                242.0  \n",
       "              253                271.0  \n",
       "              254                262.0  \n",
       "              255                252.0  \n",
       "\n",
       "[32637440 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = freq_df.index.to_frame()\n",
    "freq_df[frame['subject'].isin(train_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8b2b2bd",
   "metadata": {
    "id": "c8b2b2bd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_subject \u001b[38;5;129;01min\u001b[39;00m subjects:\n\u001b[0;32m      6\u001b[0m     train_subjects \u001b[38;5;241m=\u001b[39m subjects \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m([test_subject])\n\u001b[1;32m----> 7\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[43mall_df\u001b[49m\u001b[38;5;241m.\u001b[39mloc[train_subjects]\u001b[38;5;241m.\u001b[39mdroplevel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m all_df\u001b[38;5;241m.\u001b[39mloc[test_subject]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "total_cf_matrix = np.zeros((2,2))\n",
    "\n",
    "for test_subject in subjects:\n",
    "    train_subjects = subjects - set([test_subject])\n",
    "    train_df = all_df.loc[train_subjects].droplevel('subject')\n",
    "    test_df = all_df.loc[test_subject]\n",
    "    break\n",
    "\n",
    "    X_train = train_df.values.reshape((int(train_df.shape[0] / WINDOW_LENGTH), WINDOW_LENGTH, train_df.shape[1]))\n",
    "    X_test = test_df.values.reshape((int(test_df.shape[0] / WINDOW_LENGTH), WINDOW_LENGTH, test_df.shape[1]))\n",
    "    y_train = all_y.loc[train_subjects].reset_index(drop=True)\n",
    "    y_test = all_y.loc[test_subject].reset_index(drop=True)\n",
    "\n",
    "    X_train_under, y_train_under = random_undersampling(X_train, y_train)\n",
    "\n",
    "    model = build_model(X_train_under, False)\n",
    "    run_training(X_train_under, y_train_under, X_test, y_test, model, plot=False, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    total_cf_matrix += cf_matrix\n",
    "    sensitivity, specificity = sensitivity_specificity(cf_matrix)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    print('The results for test subject {:} are Sensitivity {:.2%} and Specificity {:.2%}'.format(test_subject, sensitivity, specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fc629",
   "metadata": {
    "id": "2c2fc629"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':list(subjects), 'Sensitivity':sensitivity_scores, 'Specificity':specificity_scores})\n",
    "\n",
    "df.plot(x=\"id\", y=[\"Sensitivity\", 'Specificity'], kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TbInRf5y-6s-",
   "metadata": {
    "id": "TbInRf5y-6s-"
   },
   "outputs": [],
   "source": [
    "print('Average Sensitivity {:.2%} and Specificity {:.2%}'.format(sum(sensitivity_scores) / len(sensitivity_scores), sum(specificity_scores) / len(specificity_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h7_2tRgvrBqs",
   "metadata": {
    "id": "h7_2tRgvrBqs"
   },
   "outputs": [],
   "source": [
    "draw_confusion_matrix(total_cf_matrix)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
